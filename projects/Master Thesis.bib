
@misc{yiASFormerTransformerAction2021,
	title = {{ASFormer}: {Transformer} for {Action} {Segmentation}},
	shorttitle = {{ASFormer}},
	url = {http://arxiv.org/abs/2110.08568},
	doi = {10.48550/arXiv.2110.08568},
	abstract = {Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at {\textbackslash}url\{https://github.com/ChinaYi/ASFormer\}.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Yi, Fangqiu and Wen, Hongyu and Jiang, Tingting},
	month = oct,
	year = {2021},
	note = {arXiv:2110.08568 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Read},
	annote = {Comment: Accepted by BMVC 2021},
	file = {Preprint PDF:C\:\\Users\\Admin\\Zotero\\storage\\9UMWZBFT\\Yi et al. - 2021 - ASFormer Transformer for Action Segmentation.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\HLFJIQWV\\2110.html:text/html},
}

@misc{liuDiffusionActionSegmentation2023,
	title = {Diffusion {Action} {Segmentation}},
	url = {http://arxiv.org/abs/2303.17959},
	doi = {10.48550/arXiv.2303.17959},
	abstract = {Temporal action segmentation is crucial for understanding long-form videos. Previous works on this task commonly adopt an iterative refinement paradigm by using multi-stage models. We propose a novel framework via denoising diffusion models, which nonetheless shares the same inherent spirit of such iterative refinement. In this framework, action predictions are iteratively generated from random noise with input video features as conditions. To enhance the modeling of three striking characteristics of human actions, including the position prior, the boundary ambiguity, and the relational dependency, we devise a unified masking strategy for the conditioning inputs in our framework. Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposed method achieves superior or comparable results to state-of-the-art methods, showing the effectiveness of a generative approach for action segmentation.},
	urldate = {2025-07-03},
	publisher = {arXiv},
	author = {Liu, Daochang and Li, Qiyue and Dinh, AnhDung and Jiang, Tingting and Shah, Mubarak and Xu, Chang},
	month = aug,
	year = {2023},
	note = {arXiv:2303.17959 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Read, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: ICCV 2023},
	file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\5S7K7X79\\Liu et al. - 2023 - Diffusion Action Segmentation.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\VQPQVLV9\\2303.html:text/html},
}

@misc{xieRethinkingSpatiotemporalFeature2018,
	title = {Rethinking {Spatiotemporal} {Feature} {Learning}: {Speed}-{Accuracy} {Trade}-offs in {Video} {Classification}},
	shorttitle = {Rethinking {Spatiotemporal} {Feature} {Learning}},
	url = {http://arxiv.org/abs/1712.04851},
	doi = {10.48550/arXiv.1712.04851},
	abstract = {Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).},
	urldate = {2025-07-07},
	publisher = {arXiv},
	author = {Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
	month = jul,
	year = {2018},
	note = {arXiv:1712.04851 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Read},
	annote = {Comment: ECCV 2018 camera ready},
	file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\N77RK3AD\\Xie et al. - 2018 - Rethinking Spatiotemporal Feature Learning Speed-Accuracy Trade-offs in Video Classification.pdf:application/pdf;Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\J6VYILGN\\1712.html:text/html},
}

@misc{segalinMouseActionRecognition2021,
	title = {The {Mouse} {Action} {Recognition} {System} ({MARS}): a software pipeline for automated analysis of social behaviors in mice},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {The {Mouse} {Action} {Recognition} {System} ({MARS})},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.26.222299v2},
	doi = {10.1101/2020.07.26.222299},
	abstract = {The study of naturalistic social behavior requires quantification of animals’ interactions. This is generally done through manual annotation—a highly time consuming and tedious process. Recent advances in computer vision enable tracking the pose (posture) of freely-behaving animals. However, automatically and accurately classifying complex social behaviors remains technically challenging. We introduce the Mouse Action Recognition System (MARS), an automated pipeline for pose estimation and behavior quantification in pairs of freely interacting mice. We compare MARS’s annotations to human annotations and find that MARS’s pose estimation and behavior classification achieve human-level performance. We also release the pose and annotation datasets used to train MARS, to serve as community benchmarks and resources. Finally, we introduce the Behavior Ensemble and Neural Trajectory Observatory (BENTO), a graphical user interface for analysis of multimodal neuroscience datasets. Together, MARS and BENTO provide an end-to-end pipeline for behavior data extraction and analysis, in a package that is user-friendly and easily modifiable.},
	language = {en},
	urldate = {2025-10-02},
	publisher = {bioRxiv},
	author = {Segalin, Cristina and Williams, Jalani and Karigo, Tomomi and Hui, May and Zelikowsky, Moriel and Sun, Jennifer J. and Perona, Pietro and Anderson, David J. and Kennedy, Ann},
	month = oct,
	year = {2021},
	note = {Pages: 2020.07.26.222299
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\XL6DVRGK\\Segalin et al. - 2021 - The Mouse Action Recognition System (MARS) a software pipeline for automated analysis of social beh.pdf:application/pdf},
}

@article{sunMultiAgentBehaviorDataset2021b,
	title = {The {Multi}-{Agent} {Behavior} {Dataset}: {Mouse} {Dyadic} {Social} {Interactions}},
	volume = {2021},
	issn = {1049-5258},
	shorttitle = {The {Multi}-{Agent} {Behavior} {Dataset}},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11067713/},
	abstract = {Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. To help accelerate behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the performance of automated behavior classification methods in three settings: (1) for training on large behavioral datasets all annotated by a single annotator, (2) for style transfer to learn inter-annotator differences in behavior definitions, and (3) for learning of new behaviors of interest given limited training data. The dataset consists of 6 million frames of unlabeled tracked poses of interacting mice, as well as over 1 million frames with tracked poses and corresponding frame-level behavior annotations. The challenge of our dataset is to be able to classify behaviors accurately using both labeled and unlabeled tracking data, as well as being able to generalize to new settings.},
	number = {DB1},
	urldate = {2025-09-27},
	journal = {Advances in neural information processing systems},
	author = {Sun, Jennifer J. and Karigo, Tomomi and Chakraborty, Dipam and Mohanty, Sharada P. and Wild, Benjamin and Sun, Quan and Chen, Chen and Anderson, David J. and Perona, Pietro and Yue, Yisong and Kennedy, Ann},
	month = dec,
	year = {2021},
	pmid = {38706835},
	pmcid = {PMC11067713},
	pages = {1--15},
}

@article{nathUsingDeepLabCut3D2019a,
	title = {Using {DeepLabCut} for {3D} markerless pose estimation across species and behaviors},
	volume = {14},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1750-2799},
	url = {https://www.nature.com/articles/s41596-019-0176-0},
	doi = {10.1038/s41596-019-0176-0},
	abstract = {Noninvasive behavioral tracking of animals during experiments is critical to many scientific pursuits. Extracting the poses of animals without using markers is often essential to measuring behavioral effects in biomechanics, genetics, ethology, and neuroscience. However, extracting detailed poses without markers in dynamically changing backgrounds has been challenging. We recently introduced an open-source toolbox called DeepLabCut that builds on a state-of-the-art human pose-estimation algorithm to allow a user to train a deep neural network with limited training data to precisely track user-defined features that match human labeling accuracy. Here, we provide an updated toolbox, developed as a Python package, that includes new features such as graphical user interfaces (GUIs), performance improvements, and active-learning-based network refinement. We provide a step-by-step procedure for using DeepLabCut that guides the user in creating a tailored, reusable analysis pipeline with a graphical processing unit (GPU) in 1–12 h (depending on frame size). Additionally, we provide Docker environments and Jupyter Notebooks that can be run on cloud resources such as Google Colaboratory.},
	language = {en},
	number = {7},
	urldate = {2025-10-07},
	journal = {Nature Protocols},
	author = {Nath, Tanmay and Mathis, Alexander and Chen, An Chi and Patel, Amir and Bethge, Matthias and Mathis, Mackenzie Weygandt},
	month = jul,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Behavioural methods, Software, Learning algorithms, Computational platforms and environments},
	pages = {2152--2176},
	file = {Full Text PDF:C\:\\Users\\Admin\\Zotero\\storage\\XWHPLCTU\\Nath et al. - 2019 - Using DeepLabCut for 3D markerless pose estimation across species and behaviors.pdf:application/pdf},
}

@misc{iashinVideoFeatures2020,
	title = {Video {Features}},
	url = {https://github.com/v-iashin/video_features},
	urldate = {2025-02-01},
	author = {Iashin, Vladimir},
	year = {2020},
}

@misc{sirmpilatzeNeuroinformaticsunitMovement2024,
	title = {neuroinformatics-unit/movement},
	shorttitle = {neuroinformatics-unit/movement},
	url = {https://zenodo.org/records/17160903},
	urldate = {2025-10-08},
	publisher = {Zenodo},
	author = {Sirmpilatze, Nikoloz and Huan, Chang and Miñano, Sofía and Brandon, D. Peri and Sharma, Dhruv and Porta, Laura and Varela, Iván and Tyson, Adam L.},
	year = {2024},
	doi = {10.5281/zenodo.17160903},
	file = {Snapshot:C\:\\Users\\Admin\\Zotero\\storage\\WIMRE2P7\\17160903.html:text/html},
}
