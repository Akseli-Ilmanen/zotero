@misc{3blue1brownAttentionTransformersStepbystep2024,
  title = {Attention in Transformers, Step-by-Step {\textbar} {{Deep Learning Chapter}} 6},
  author = {{3Blue1Brown}},
  year = 2024,
  month = apr,
  urldate = {2025-10-27},
  abstract = {Demystifying attention, the key mechanism inside transformers and LLMs. Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support Special thanks to these supporters: https://www.3blue1brown.com/lessons/a...}
}

@misc{3blue1brownTransformersTechLLMs2024a,
  title = {Transformers, the Tech behind {{LLMs}} {\textbar} {{Deep Learning Chapter}} 5},
  author = {{3Blue1Brown}},
  year = 2024,
  month = apr,
  urldate = {2025-10-27},
  abstract = {Breaking down how Large Language Models work, visualizing how data flows through. Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support}
}

@misc{cambridgeneurotechNeuralProbeData2025,
  type = {Text/Html},
  title = {Neural {{Probe Data}}},
  author = {Cambridge NeuroTech},
  year = 2025,
  month = oct,
  publisher = {Cambridge NeuroTech},
  urldate = {2025-10-27},
  abstract = {Advanced neural interfacing technology (silicon neural probes) for pre-clinical research covering neuroscience, neuroprosthetics and brain-machine interfaces.},
  copyright = {Copyright {\copyright}2025 Cambridge NeuroTech.},
  howpublished = {https://www.cambridgeneurotech.com/neural-probes/neural-probe-data},
  langid = {english},
  file = {C:\Users\Admin\Zotero\storage\K3CDKGR2\neural-probe-data.html}
}

@article{friardBORISFreeVersatile2016,
  title = {{{BORIS}}: A Free, Versatile Open-Source Event-Logging Software for Video/Audio Coding and Live Observations},
  shorttitle = {{{BORIS}}},
  author = {Friard, Olivier and Gamba, Marco},
  year = 2016,
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {11},
  pages = {1325--1330},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12584},
  urldate = {2025-10-13},
  abstract = {Quantitative aspects of the study of animal and human behaviour are increasingly relevant to test hypotheses and find empirical support for them. At the same time, photo and video cameras can store a large number of video recordings and are often used to monitor the subjects remotely. Researchers frequently face the need to code considerable quantities of video recordings with relatively flexible software, often constrained by species-specific options or exact settings. BORIS is a free, open-source and multiplatform standalone program that allows a user-specific coding environment to be set for a computer-based review of previously recorded videos or live observations. Being open to user-specific settings, the program allows a project-based ethogram to be defined that can then be shared with collaborators, or can be imported or modified. Projects created in BORIS can include a list of observations, and each observation may include one or two videos (e.g. simultaneous screening of visual stimuli and the subject being tested; recordings from different sides of an aquarium). Once the user has set an ethogram, including state or point events or both, coding can be performed using previously assigned keys on the computer keyboard. BORIS allows definition of an unlimited number of events (states/point events) and subjects. Once the coding process is completed, the program can extract a time-budget or single or grouped observations automatically and present an at-a-glance summary of the main behavioural features. The observation data and time-budget analysis can be exported in many common formats (TSV, CSV, ODF, XLS, SQL and JSON). The observed events can be plotted and exported in various graphic formats (SVG, PNG, JPG, TIFF, EPS and PDF).},
  copyright = {{\copyright} 2016 The Authors. Methods in Ecology and Evolution {\copyright} 2016 British Ecological Society},
  langid = {english},
  keywords = {behaviour coding,behavioural analysis,coding scheme,ethology,observational data,ttime-budget},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LJ8ZEVGT\\Friard and Gamba - 2016 - BORIS a free, versatile open-source event-logging software for videoaudio coding and live observat.pdf;C\:\\Users\\Admin\\Zotero\\storage\\59UXDLT4\\2041-210X.html}
}

@article{hsuBSOiDOpensourceUnsupervised2021a,
  title = {B-{{SOiD}}, an Open-Source Unsupervised Algorithm for Identification and Fast Prediction of Behaviors},
  author = {Hsu, Alexander I. and Yttri, Eric A.},
  year = 2021,
  month = aug,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5188},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25420-x},
  urldate = {2025-10-30},
  abstract = {Studying naturalistic animal behavior remains a difficult objective. Recent machine learning advances have enabled limb localization; however, extracting behaviors requires ascertaining the spatiotemporal patterns of these positions. To provide a link from poses to actions and their kinematics, we developed B-SOiD -~an open-source, unsupervised algorithm that identifies behavior without user bias. By training a machine classifier on pose pattern statistics clustered using new methods, our approach achieves greatly improved processing speed and the ability to generalize across subjects or labs. Using a frameshift alignment paradigm, B-SOiD overcomes previous temporal resolution barriers. Using only a single, off-the-shelf camera, B-SOiD provides categories of sub-action for trained behaviors and kinematic measures of individual limb trajectories in any animal model. These behavioral and kinematic measures are difficult but critical to obtain, particularly in the study of rodent and other models of pain, OCD, and movement disorders.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Behavioural methods,Diseases of the nervous system,Motor control},
  file = {C:\Users\Admin\Zotero\storage\YFX7N6FP\Hsu and Yttri - 2021 - B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors.pdf}
}

@misc{iashinVideoFeatures2020,
  title = {Video {{Features}}},
  author = {Iashin, Vladimir},
  year = 2020,
  urldate = {2025-02-01}
}

@misc{kozlovaDLC2ActionDeepLearningbased2025,
  title = {{{DLC2Action}}: {{A Deep Learning-based Toolbox}} for {{Automated Behavior Segmentation}}},
  shorttitle = {{{DLC2Action}}},
  author = {Kozlova, Elizaveta and Bonnetto, Andy and Mathis, Alexander},
  year = 2025,
  month = sep,
  primaryclass = {New Results},
  pages = {2025.09.27.678941},
  publisher = {bioRxiv},
  issn = {2692-8205},
  doi = {10.1101/2025.09.27.678941},
  urldate = {2025-10-28},
  abstract = {While expert biologists can annotate complex behaviors from video data, the process remains tedious and time-consuming, creating a bottleneck for efficient behavioral analysis. Here, we present DLC2Action, an open-source Python toolbox that enables automatic behavior annotation from video or estimated 2D/3D pose tracking data. DLC2Action integrates multiple state-of-the-art deep learning architectures, optimized for action segmentation and supports self-supervised learning (SSL) to leverage unlabeled data, boosting performance with limited labeled datasets. Its robust implementation enables efficient hyperparameter optimization, customizable feature extraction, and data handling. We also standardized eight benchmarks and evaluated DLC2Action on five animal behavior datasets, which comprise common behavioral tests in neuroscience, and four human datasets. Overall these datasets span a wide range of contexts from standard laboratory studies to naturalistic cooking. DLC2Action reached strong performance across those benchmarks. To further showcase the tool's versatility, we applied it to Atari gameplay data and found that in certain games the players' eye movements consistently predict their button presses across different subjects. Furthermore, DLC2Action features an intuitive graphical user interface (GUI) for streamlined behavior annotation, active learning, and assessment of model predictions. Diverse pose, video, and annotation formats are supported. Lastly, DLC2Action is modular and thus designed for extensibility, allowing users to integrate new models, dataset features, and methods. The code and benchmarks are available at: https://github.com/amathislab/DLC2action},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {C:\Users\Admin\Zotero\storage\5ZIW25C9\Kozlova et al. - 2025 - DLC2Action A Deep Learning-based Toolbox for Automated Behavior Segmentation.pdf}
}

@article{linCharacterizingStructureMouse2024,
  title = {Characterizing the Structure of Mouse Behavior Using {{Motion Sequencing}}},
  author = {Lin, Sherry and Gillis, Winthrop F. and Weinreb, Caleb and Zeine, Ayman and Jones, Samuel C. and Robinson, Emma M. and Markowitz, Jeffrey and Datta, Sandeep Robert},
  year = 2024,
  month = nov,
  journal = {Nature Protocols},
  volume = {19},
  number = {11},
  pages = {3242--3291},
  publisher = {Nature Publishing Group},
  issn = {1750-2799},
  doi = {10.1038/s41596-024-01015-w},
  urldate = {2025-10-30},
  abstract = {Spontaneous mouse behavior is composed from repeatedly used modules of movement (e.g., rearing, running or grooming) that are flexibly placed into sequences whose content evolves over time. By identifying behavioral modules and the order in which they are expressed, researchers can gain insight into the effect of drugs, genes, context, sensory stimuli and neural activity on natural behavior. Here we present a protocol for performing Motion Sequencing (MoSeq), an ethologically inspired method that uses three-dimensional machine vision and unsupervised machine learning to decompose spontaneous mouse behavior into a series of elemental modules called `syllables'. This protocol is based upon a MoSeq pipeline that includes modules for depth video acquisition, data preprocessing and modeling, as well as a standardized set of visualization tools. Users are provided with instructions and code for building a MoSeq imaging rig and acquiring three-dimensional video of spontaneous mouse behavior for submission to the modeling framework; the outputs of this protocol include syllable labels for each frame of the video data as well as summary plots describing how often each syllable was used and how syllables transitioned from one to the other. In addition, we provide instructions for analyzing and visualizing the outputs of keypoint-MoSeq, a recently developed variant of MoSeq that can identify behavioral motifs from keypoints identified from standard (rather than depth) video. This protocol and the accompanying pipeline significantly lower the bar for users without extensive computational ethology experience to adopt this unsupervised, data-driven approach to characterize mouse behavior.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Behavioural methods,Neuroscience},
  file = {C:\Users\Admin\Zotero\storage\S5MQVT3U\Lin et al. - 2024 - Characterizing the structure of mouse behavior using Motion Sequencing.pdf}
}

@misc{liuDiffusionActionSegmentation2023,
  title = {Diffusion {{Action Segmentation}}},
  author = {Liu, Daochang and Li, Qiyue and Dinh, AnhDung and Jiang, Tingting and Shah, Mubarak and Xu, Chang},
  year = 2023,
  month = aug,
  number = {arXiv:2303.17959},
  eprint = {2303.17959},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17959},
  urldate = {2025-07-03},
  abstract = {Temporal action segmentation is crucial for understanding long-form videos. Previous works on this task commonly adopt an iterative refinement paradigm by using multi-stage models. We propose a novel framework via denoising diffusion models, which nonetheless shares the same inherent spirit of such iterative refinement. In this framework, action predictions are iteratively generated from random noise with input video features as conditions. To enhance the modeling of three striking characteristics of human actions, including the position prior, the boundary ambiguity, and the relational dependency, we devise a unified masking strategy for the conditioning inputs in our framework. Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposed method achieves superior or comparable results to state-of-the-art methods, showing the effectiveness of a generative approach for action segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,Read},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5S7K7X79\\Liu et al. - 2023 - Diffusion Action Segmentation.pdf;C\:\\Users\\Admin\\Zotero\\storage\\VQPQVLV9\\2303.html}
}

@article{luxemIdentifyingBehavioralStructure2022,
  title = {Identifying Behavioral Structure from Deep Variational Embeddings of Animal Motion},
  author = {Luxem, Kevin and Mocellin, Petra and Fuhrmann, Falko and K{\"u}rsch, Johannes and Miller, Stephanie R. and Palop, Jorge J. and Remy, Stefan and Bauer, Pavol},
  year = 2022,
  month = nov,
  journal = {Communications Biology},
  volume = {5},
  number = {1},
  pages = {1267},
  publisher = {Nature Publishing Group},
  issn = {2399-3642},
  doi = {10.1038/s42003-022-04080-7},
  urldate = {2025-10-30},
  abstract = {Quantification and detection of the hierarchical organization of behavior is a major challenge in neuroscience. Recent advances in markerless pose estimation enable the visualization of high-dimensional spatiotemporal behavioral dynamics of animal motion. However, robust and reliable technical approaches are needed to uncover underlying structure in these data and to segment behavior into discrete hierarchically organized motifs. Here, we present an unsupervised probabilistic deep learning framework that identifies behavioral structure from deep variational embeddings of animal motion (VAME). By using a mouse model of beta amyloidosis as a use case, we show that VAME not only identifies discrete behavioral motifs, but also captures a hierarchical representation of the motif's usage. The approach allows for the grouping of motifs into communities and the detection of differences in community-specific motif usage of individual mouse cohorts that were undetectable by human visual observation. Thus, we present a robust approach for the segmentation of animal motion that is applicable to a wide range of experimental setups, models and conditions without requiring supervised or a-priori human interference.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Behavioural methods,Computational neuroscience},
  file = {C:\Users\Admin\Zotero\storage\LY8DMF3P\Luxem et al. - 2022 - Identifying behavioral structure from deep variational embeddings of animal motion.pdf}
}

@article{luxemOpensourceToolsBehavioral,
  title = {Open-Source Tools for Behavioral Video Analysis: {{Setup}}, Methods, and Best Practices},
  shorttitle = {Open-Source Tools for Behavioral Video Analysis},
  author = {Luxem, Kevin and Sun, Jennifer J and Bradley, Sean P and Krishnan, Keerthi and Yttri, Eric and Zimmermann, Jan and Pereira, Talmo D and Laubach, Mark},
  journal = {eLife},
  volume = {12},
  pages = {e79305},
  issn = {2050-084X},
  doi = {10.7554/eLife.79305},
  urldate = {2025-03-22},
  abstract = {Recently developed methods for video analysis, especially models for pose estimation and behavior classification, are transforming behavioral quantification to be more precise, scalable, and reproducible in fields such as neuroscience and ethology. These tools overcome long-standing limitations of manual scoring of video frames and traditional `center of mass' tracking algorithms to enable video analysis at scale. The expansion of open-source tools for video acquisition and analysis has led to new experimental approaches to understand behavior. Here, we review currently available open-source tools for video analysis and discuss how to set up these methods for labs new to video recording. We also discuss best practices for developing and using video analysis methods, including community-wide standards and critical needs for the open sharing of datasets and code, more widespread comparisons of video analysis methods, and better documentation for these methods especially for new users. We encourage broader adoption and continued development of these tools, which have tremendous potential for accelerating scientific progress in understanding the brain and behavior.},
  pmcid = {PMC10036114},
  pmid = {36951911},
  keywords = {Maybe Read},
  file = {C:\Users\Admin\Zotero\storage\XNIVU9Q7\Luxem et al. - Open-source tools for behavioral video analysis Setup, methods, and best practices.pdf}
}

@article{mlostEvaluationUnsupervisedLearning2025,
  title = {Evaluation of Unsupervised Learning Algorithms for the Classification of Behavior from Pose Estimation Data},
  author = {Mlost, Jakub and Dawli, Rame and Liu, Xuan and Costa, Ana Rita and Dorocic, Iskra Pollak},
  year = 2025,
  month = may,
  journal = {Patterns},
  volume = {6},
  number = {5},
  publisher = {Elsevier},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2025.101237},
  urldate = {2025-05-15},
  langid = {english},
  keywords = {behavioral classification,Must Read,neuroethology,neuroscience,unsupervised learning},
  file = {C:\Users\Admin\Zotero\storage\7LZEFNWD\Mlost et al. - 2025 - Evaluation of unsupervised learning algorithms for the classification of behavior from pose estimati.pdf}
}

@article{mollLearnedPrecisionTool2025,
  title = {Learned Precision Tool Use in Carrion Crows},
  author = {Moll, Felix W. and W{\"u}rzler, Julius and Nieder, Andreas},
  year = 2025,
  month = oct,
  journal = {Current Biology},
  volume = {35},
  number = {19},
  pages = {4845-4852.e3},
  publisher = {Elsevier},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2025.08.033},
  urldate = {2025-10-13},
  langid = {english},
  pmid = {40934918},
  keywords = {carrion crow,corvid,Corvus corone,pose estimation,skill learning,tool use},
  file = {C:\Users\Admin\Zotero\storage\C6ZHWUI8\Moll et al. - 2025 - Learned precision tool use in carrion crows.pdf}
}

@article{nathUsingDeepLabCut3D2019a,
  title = {Using {{DeepLabCut}} for {{3D}} Markerless Pose Estimation across Species and Behaviors},
  author = {Nath, Tanmay and Mathis, Alexander and Chen, An Chi and Patel, Amir and Bethge, Matthias and Mathis, Mackenzie Weygandt},
  year = 2019,
  month = jul,
  journal = {Nature Protocols},
  volume = {14},
  number = {7},
  pages = {2152--2176},
  publisher = {Nature Publishing Group},
  issn = {1750-2799},
  doi = {10.1038/s41596-019-0176-0},
  urldate = {2025-10-07},
  abstract = {Noninvasive behavioral tracking of animals during experiments is critical to many scientific pursuits. Extracting the poses of animals without using markers is often essential to measuring behavioral effects in biomechanics, genetics, ethology, and neuroscience. However, extracting detailed poses without markers in dynamically changing backgrounds has been challenging. We recently introduced an open-source toolbox called DeepLabCut that builds on a state-of-the-art human pose-estimation algorithm to allow a user to train a deep neural network with limited training data to precisely track user-defined features that match human labeling accuracy. Here, we provide an updated toolbox, developed as a Python package, that includes new features such as graphical user interfaces (GUIs), performance improvements, and active-learning-based network refinement. We provide a step-by-step procedure for using DeepLabCut that guides the user in creating a tailored, reusable analysis pipeline with a graphical processing unit (GPU) in 1--12 h (depending on frame size). Additionally, we provide Docker environments and Jupyter Notebooks that can be run on cloud resources such as Google Colaboratory.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Behavioural methods,Computational platforms and environments,Learning algorithms,Software},
  file = {C:\Users\Admin\Zotero\storage\XWHPLCTU\Nath et al. - 2019 - Using DeepLabCut for 3D markerless pose estimation across species and behaviors.pdf}
}

@article{pachitariuSpikeSortingKilosort42024a,
  title = {Spike Sorting with {{Kilosort4}}},
  author = {Pachitariu, Marius and Sridhar, Shashwat and Pennington, Jacob and Stringer, Carsen},
  year = 2024,
  month = may,
  journal = {Nature Methods},
  volume = {21},
  number = {5},
  pages = {914--921},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02232-7},
  urldate = {2025-10-27},
  abstract = {Spike sorting is the computational process of extracting the firing times of single neurons from recordings of local electrical fields. This is an important but hard problem in neuroscience, made complicated by the nonstationarity of the recordings and the dense overlap in electrical fields between nearby neurons. To address the spike-sorting problem, we have been openly developing the Kilosort framework. Here we describe the various algorithmic steps introduced in different versions of Kilosort. We also report the development of Kilosort4, a version with substantially improved performance due to clustering algorithms inspired by graph-based approaches. To test the performance of Kilosort, we developed a realistic simulation framework that uses densely sampled electrical fields from real experiments to generate nonstationary spike waveforms and realistic noise. We found that nearly all versions of Kilosort outperformed other algorithms on a variety of simulated conditions and that Kilosort4 performed best in all cases, correctly identifying even neurons with low amplitudes and small spatial extents in high drift conditions.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Computational platforms and environments},
  file = {C:\Users\Admin\Zotero\storage\H36Y7FCY\Pachitariu et al. - 2024 - Spike sorting with Kilosort4.pdf}
}

@misc{segalinMouseActionRecognition2021,
  title = {The {{Mouse Action Recognition System}} ({{MARS}}): A Software Pipeline for Automated Analysis of Social Behaviors in Mice},
  shorttitle = {The {{Mouse Action Recognition System}} ({{MARS}})},
  author = {Segalin, Cristina and Williams, Jalani and Karigo, Tomomi and Hui, May and Zelikowsky, Moriel and Sun, Jennifer J. and Perona, Pietro and Anderson, David J. and Kennedy, Ann},
  year = 2021,
  month = oct,
  primaryclass = {New Results},
  pages = {2020.07.26.222299},
  publisher = {bioRxiv},
  doi = {10.1101/2020.07.26.222299},
  urldate = {2025-10-02},
  abstract = {The study of naturalistic social behavior requires quantification of animals' interactions. This is generally done through manual annotation---a highly time consuming and tedious process. Recent advances in computer vision enable tracking the pose (posture) of freely-behaving animals. However, automatically and accurately classifying complex social behaviors remains technically challenging. We introduce the Mouse Action Recognition System (MARS), an automated pipeline for pose estimation and behavior quantification in pairs of freely interacting mice. We compare MARS's annotations to human annotations and find that MARS's pose estimation and behavior classification achieve human-level performance. We also release the pose and annotation datasets used to train MARS, to serve as community benchmarks and resources. Finally, we introduce the Behavior Ensemble and Neural Trajectory Observatory (BENTO), a graphical user interface for analysis of multimodal neuroscience datasets. Together, MARS and BENTO provide an end-to-end pipeline for behavior data extraction and analysis, in a package that is user-friendly and easily modifiable.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {C:\Users\Admin\Zotero\storage\XL6DVRGK\Segalin et al. - 2021 - The Mouse Action Recognition System (MARS) a software pipeline for automated analysis of social beh.pdf}
}

@misc{sirmpilatzeNeuroinformaticsunitMovement2024,
  title = {Neuroinformatics-Unit/Movement},
  shorttitle = {Neuroinformatics-Unit/Movement},
  author = {Sirmpilatze, Nikoloz and Huan, Chang and Mi{\~n}ano, Sof{\'i}a and Brandon, D. Peri and Sharma, Dhruv and Porta, Laura and Varela, Iv{\'a}n and Tyson, Adam L.},
  year = 2024,
  doi = {10.5281/zenodo.17160903},
  urldate = {2025-10-08},
  howpublished = {Zenodo},
  file = {C:\Users\Admin\Zotero\storage\WIMRE2P7\17160903.html}
}

@article{steinmetzNeuropixels20Miniaturized2021a,
  title = {Neuropixels 2.0: {{A}} Miniaturized High-Density Probe for Stable, Long-Term Brain Recordings},
  shorttitle = {Neuropixels 2.0},
  author = {Steinmetz, Nicholas A. and Aydin, Cagatay and Lebedeva, Anna and Okun, Michael and Pachitariu, Marius and Bauza, Marius and Beau, Maxime and Bhagat, Jai and B{\"o}hm, Claudia and Broux, Martijn and Chen, Susu and Colonell, Jennifer and Gardner, Richard J. and Karsh, Bill and Kloosterman, Fabian and Kostadinov, Dimitar and {Mora-Lopez}, Carolina and O'Callaghan, John and Park, Junchol and Putzeys, Jan and Sauerbrei, Britton and {van Daal}, Rik J. J. and Vollan, Abraham Z. and Wang, Shiwei and Welkenhuysen, Marleen and Ye, Zhiwen and Dudman, Joshua T. and Dutta, Barundeb and Hantman, Adam W. and Harris, Kenneth D. and Lee, Albert K. and Moser, Edvard I. and O'Keefe, John and Renart, Alfonso and Svoboda, Karel and H{\"a}usser, Michael and Haesler, Sebastian and Carandini, Matteo and Harris, Timothy D.},
  year = 2021,
  month = apr,
  journal = {Science},
  volume = {372},
  number = {6539},
  pages = {eabf4588},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.abf4588},
  urldate = {2025-10-27},
  abstract = {Measuring the dynamics of neural processing across time scales requires following the spiking of thousands of individual neurons over milliseconds and months. To address this need, we introduce the Neuropixels 2.0 probe together with newly designed analysis algorithms. The probe has more than 5000 sites and is miniaturized to facilitate chronic implants in small mammals and recording during unrestrained behavior. High-quality recordings over long time scales were reliably obtained in mice and rats in six laboratories. Improved site density and arrangement combined with newly created data processing methods enable automatic post hoc correction for brain movements, allowing recording from the same neurons for more than 2 months. These probes and algorithms enable stable recordings from thousands of sites during free behavior, even in small animals such as mice.},
  file = {C:\Users\Admin\Zotero\storage\4NFD7DYF\Steinmetz et al. - 2021 - Neuropixels 2.0 A miniaturized high-density probe for stable, long-term brain recordings.pdf}
}

@article{sunMultiAgentBehaviorDataset2021b,
  title = {The {{Multi-Agent Behavior Dataset}}: {{Mouse Dyadic Social Interactions}}},
  shorttitle = {The {{Multi-Agent Behavior Dataset}}},
  author = {Sun, Jennifer J. and Karigo, Tomomi and Chakraborty, Dipam and Mohanty, Sharada P. and Wild, Benjamin and Sun, Quan and Chen, Chen and Anderson, David J. and Perona, Pietro and Yue, Yisong and Kennedy, Ann},
  year = 2021,
  month = dec,
  journal = {Advances in neural information processing systems},
  volume = {2021},
  number = {DB1},
  pages = {1--15},
  issn = {1049-5258},
  urldate = {2025-09-27},
  abstract = {Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. To help accelerate behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the performance of automated behavior classification methods in three settings: (1) for training on large behavioral datasets all annotated by a single annotator, (2) for style transfer to learn inter-annotator differences in behavior definitions, and (3) for learning of new behaviors of interest given limited training data. The dataset consists of 6 million frames of unlabeled tracked poses of interacting mice, as well as over 1 million frames with tracked poses and corresponding frame-level behavior annotations. The challenge of our dataset is to be able to classify behaviors accurately using both labeled and unlabeled tracking data, as well as being able to generalize to new settings.},
  pmcid = {PMC11067713},
  pmid = {38706835}
}

@article{tillmannASOiDActivelearningPlatform2024a,
  title = {A-{{SOiD}}, an Active-Learning Platform for Expert-Guided, Data-Efficient Discovery of Behavior},
  author = {Tillmann, Jens F. and Hsu, Alexander I. and Schwarz, Martin K. and Yttri, Eric A.},
  year = 2024,
  month = apr,
  journal = {Nature Methods},
  volume = {21},
  number = {4},
  pages = {703--711},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02200-1},
  urldate = {2025-10-30},
  abstract = {To identify and extract naturalistic behavior, two methods have become popular: supervised and unsupervised. Each approach carries its own strengths and weaknesses (for example, user bias, training cost, complexity and action discovery), which the user must consider in their decision. Here, an active-learning platform, A-SOiD, blends these strengths, and in doing so, overcomes several of their inherent drawbacks. A-SOiD iteratively learns user-defined groups with a fraction of the usual training data, while attaining expansive classification through directed unsupervised classification. In socially interacting mice, A-SOiD outperformed standard methods despite requiring 85\% less training data. Additionally, it isolated ethologically distinct mouse interactions via unsupervised classification. We observed similar performance and efficiency using nonhuman primate and human three-dimensional pose data. In both cases, the transparency in A-SOiD's cluster definitions revealed the defining features of the supervised classification through a game-theoretic approach. To facilitate use, A-SOiD comes as an intuitive, open-source interface for efficient segmentation of user-defined behaviors and discovered sub-actions.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Behavioural methods,Software},
  file = {C:\Users\Admin\Zotero\storage\AUDE3PQA\Tillmann et al. - 2024 - A-SOiD, an active-learning platform for expert-guided, data-efficient discovery of behavior.pdf}
}

@article{tillmannSupplementaryASOiDActivelearning2024,
  title = {Supplementary: {{A-SOiD}}, an Active-Learning Platform for Expert-Guided, Data-Efficient Discovery of Behavior},
  author = {Tillmann, Jens F. and Hsu, Alexander I. and Schwarz, Martin K. and Yttri, Eric A.},
  year = 2024,
  month = apr,
  journal = {Nature Methods},
  volume = {21},
  number = {4},
  pages = {703--711},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-024-02200-1},
  urldate = {2025-06-03},
  langid = {english},
  keywords = {Read},
  file = {C:\Users\Admin\Zotero\storage\8LZRFRQK\Tillmann et al. - 2024 - A-SOiD, an active-learning platform for expert-guided, data-efficient discovery of behavior.pdf}
}

@article{torricelliMotorInvariantsAction2023,
  title = {Motor Invariants in Action Execution and Perception},
  author = {Torricelli, Francesco and Tomassini, Alice and Pezzulo, Giovanni and Pozzo, Thierry and Fadiga, Luciano and D'Ausilio, Alessandro},
  year = 2023,
  month = mar,
  journal = {Physics of Life Reviews},
  volume = {44},
  pages = {13--47},
  issn = {15710645},
  doi = {10.1016/j.plrev.2022.11.003},
  urldate = {2025-02-17},
  abstract = {The nervous system is sensitive to statistical regularities of the external world and forms internal models of these regularities to predict environmental dynamics. Given the inherently social nature of human behavior, being capable of building reliable predictive models of others' actions may be essential for successful interaction. While social prediction might seem to be a daunting task, the study of human motor control has accumulated ample evidence that our movements follow a series of kinematic invariants, which can be used by observers to reduce their uncertainty during social exchanges. Here, we provide an overview of the most salient regularities that shape biological motion, examine the role of these invariants in recognizing others' actions, and speculate that anchoring socially-relevant perceptual decisions to such kinematic invariants provides a key computational advantage for inferring conspecifics' goals and intentions.},
  langid = {english},
  keywords = {Read},
  file = {C:\Users\Admin\Zotero\storage\WBFAZLDK\Torricelli et al. - 2023 - Motor invariants in action execution and perception.pdf}
}

@article{vonzieglerAnalysisBehavioralFlow2024a,
  title = {Analysis of Behavioral Flow Resolves Latent Phenotypes},
  author = {{von Ziegler}, Lukas M. and Roessler, Fabienne K. and Sturman, Oliver and Waag, Rebecca and Privitera, Mattia and Duss, Sian N. and O'Connor, Eoin C. and Bohacek, Johannes},
  year = 2024,
  month = dec,
  journal = {Nature Methods},
  volume = {21},
  number = {12},
  pages = {2376--2387},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02500-6},
  urldate = {2025-10-30},
  abstract = {The accurate detection and quantification of rodent behavior forms a cornerstone of basic biomedical research. Current data-driven approaches, which segment free exploratory behavior into clusters, suffer from low statistical power due to multiple testing, exhibit poor transferability across experiments and fail to exploit the rich behavioral profiles of individual animals. Here we introduce a pipeline to capture each animal's behavioral flow, yielding a single metric based on all observed transitions between clusters. By stabilizing these clusters through machine learning, we ensure data transferability, while dimensionality reduction techniques facilitate detailed analysis of individual animals. We provide a large dataset of 771 behavior recordings of freely moving mice---including stress exposures, pharmacological and brain circuit interventions---to identify hidden treatment effects, reveal subtle variations on the level of individual animals and detect brain processes underlying specific interventions. Our pipeline, compatible with popular clustering methods, substantially enhances statistical power and enables predictions of an animal's future behavior.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Behavioural methods,Diseases of the nervous system,Emotion,Mouse,Software},
  file = {C:\Users\Admin\Zotero\storage\HJIXB2QD\von Ziegler et al. - 2024 - Analysis of behavioral flow resolves latent phenotypes.pdf}
}

@misc{xieRethinkingSpatiotemporalFeature2018,
  title = {Rethinking {{Spatiotemporal Feature Learning}}: {{Speed-Accuracy Trade-offs}} in {{Video Classification}}},
  shorttitle = {Rethinking {{Spatiotemporal Feature Learning}}},
  author = {Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
  year = 2018,
  month = jul,
  number = {arXiv:1712.04851},
  eprint = {1712.04851},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.04851},
  urldate = {2025-07-07},
  abstract = {Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Read},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\N77RK3AD\\Xie et al. - 2018 - Rethinking Spatiotemporal Feature Learning Speed-Accuracy Trade-offs in Video Classification.pdf;C\:\\Users\\Admin\\Zotero\\storage\\J6VYILGN\\1712.html}
}

@article{xuChangepointDetectionDeep2025,
  title = {Change-Point Detection with Deep Learning: {{A}} Review},
  shorttitle = {Change-Point Detection with Deep Learning},
  author = {Xu, Ruiyu and Song, Zheren and Wu, Jianguo and Wang, Chao and Zhou, Shiyu},
  year = 2025,
  month = mar,
  journal = {Frontiers of Engineering Management},
  volume = {12},
  number = {1},
  pages = {154--176},
  issn = {2096-0255},
  doi = {10.1007/s42524-025-4109-z},
  urldate = {2025-10-14},
  abstract = {Recent advances in deep learning have led to the creation of various methods for change-point detection (CPD). These methods enhance the ability of CPD techniques to handle complex, high-dimensional data, making them more adaptable and less dependent on strict assumptions about data distributions. CPD methods have also demonstrated high accuracy and have been applied across various fields, including manufacturing, healthcare, activity monitoring, finance, and environmental monitoring. This review provides an overview of how these methods are applied, the data sets they use, and how their performance is evaluated. It also organizes techniques into supervised and unsupervised categories, citing key studies. Finally, we explore ongoing challenges and suggest directions for future research to improve interpretability, generalizability, and real-world implementation.},
  langid = {english},
  keywords = {change-point detection,deep learning,supervised learning,time-series analysis,unsupervised learning},
  file = {C:\Users\Admin\Zotero\storage\X5ZCITZ7\Xu et al. - 2025 - Change-point detection with deep learning A review.pdf}
}

@misc{yiASFormerTransformerAction2021,
  title = {{{ASFormer}}: {{Transformer}} for {{Action Segmentation}}},
  shorttitle = {{{ASFormer}}},
  author = {Yi, Fangqiu and Wen, Hongyu and Jiang, Tingting},
  year = 2021,
  month = oct,
  number = {arXiv:2110.08568},
  eprint = {2110.08568},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.08568},
  urldate = {2025-06-24},
  abstract = {Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at {\textbackslash}url\{https://github.com/ChinaYi/ASFormer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Read},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9UMWZBFT\\Yi et al. - 2021 - ASFormer Transformer for Action Segmentation.pdf;C\:\\Users\\Admin\\Zotero\\storage\\HLFJIQWV\\2110.html}
}
