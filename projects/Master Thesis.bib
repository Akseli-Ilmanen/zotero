@article{friardBORISFreeVersatile2016,
  title = {{{BORIS}}: A Free, Versatile Open-Source Event-Logging Software for Video/Audio Coding and Live Observations},
  shorttitle = {{{BORIS}}},
  author = {Friard, Olivier and Gamba, Marco},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {11},
  pages = {1325--1330},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12584},
  urldate = {2025-10-13},
  abstract = {Quantitative aspects of the study of animal and human behaviour are increasingly relevant to test hypotheses and find empirical support for them. At the same time, photo and video cameras can store a large number of video recordings and are often used to monitor the subjects remotely. Researchers frequently face the need to code considerable quantities of video recordings with relatively flexible software, often constrained by species-specific options or exact settings. BORIS is a free, open-source and multiplatform standalone program that allows a user-specific coding environment to be set for a computer-based review of previously recorded videos or live observations. Being open to user-specific settings, the program allows a project-based ethogram to be defined that can then be shared with collaborators, or can be imported or modified. Projects created in BORIS can include a list of observations, and each observation may include one or two videos (e.g. simultaneous screening of visual stimuli and the subject being tested; recordings from different sides of an aquarium). Once the user has set an ethogram, including state or point events or both, coding can be performed using previously assigned keys on the computer keyboard. BORIS allows definition of an unlimited number of events (states/point events) and subjects. Once the coding process is completed, the program can extract a time-budget or single or grouped observations automatically and present an at-a-glance summary of the main behavioural features. The observation data and time-budget analysis can be exported in many common formats (TSV, CSV, ODF, XLS, SQL and JSON). The observed events can be plotted and exported in various graphic formats (SVG, PNG, JPG, TIFF, EPS and PDF).},
  copyright = {{\copyright} 2016 The Authors. Methods in Ecology and Evolution {\copyright} 2016 British Ecological Society},
  langid = {english},
  keywords = {behaviour coding,behavioural analysis,coding scheme,ethology,observational data,ttime-budget},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LJ8ZEVGT\\Friard and Gamba - 2016 - BORIS a free, versatile open-source event-logging software for videoaudio coding and live observat.pdf;C\:\\Users\\Admin\\Zotero\\storage\\59UXDLT4\\2041-210X.html}
}

@misc{iashinVideoFeatures2020,
  title = {Video {{Features}}},
  author = {Iashin, Vladimir},
  year = {2020},
  urldate = {2025-02-01}
}

@misc{liuDiffusionActionSegmentation2023,
  title = {Diffusion {{Action Segmentation}}},
  author = {Liu, Daochang and Li, Qiyue and Dinh, AnhDung and Jiang, Tingting and Shah, Mubarak and Xu, Chang},
  year = {2023},
  month = aug,
  number = {arXiv:2303.17959},
  eprint = {2303.17959},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17959},
  urldate = {2025-07-03},
  abstract = {Temporal action segmentation is crucial for understanding long-form videos. Previous works on this task commonly adopt an iterative refinement paradigm by using multi-stage models. We propose a novel framework via denoising diffusion models, which nonetheless shares the same inherent spirit of such iterative refinement. In this framework, action predictions are iteratively generated from random noise with input video features as conditions. To enhance the modeling of three striking characteristics of human actions, including the position prior, the boundary ambiguity, and the relational dependency, we devise a unified masking strategy for the conditioning inputs in our framework. Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposed method achieves superior or comparable results to state-of-the-art methods, showing the effectiveness of a generative approach for action segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,Read},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5S7K7X79\\Liu et al. - 2023 - Diffusion Action Segmentation.pdf;C\:\\Users\\Admin\\Zotero\\storage\\VQPQVLV9\\2303.html}
}

@article{mollLearnedPrecisionTool2025,
  title = {Learned Precision Tool Use in Carrion Crows},
  author = {Moll, Felix W. and W{\"u}rzler, Julius and Nieder, Andreas},
  year = {2025},
  month = oct,
  journal = {Current Biology},
  volume = {35},
  number = {19},
  pages = {4845-4852.e3},
  publisher = {Elsevier},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2025.08.033},
  urldate = {2025-10-13},
  langid = {english},
  pmid = {40934918},
  keywords = {carrion crow,corvid,Corvus corone,pose estimation,skill learning,tool use},
  file = {C:\Users\Admin\Zotero\storage\C6ZHWUI8\Moll et al. - 2025 - Learned precision tool use in carrion crows.pdf}
}

@article{morassoSpatialControlArm1981,
  title = {Spatial Control of Arm Movements},
  author = {Morasso, P.},
  year = {1981},
  month = apr,
  journal = {Experimental Brain Research},
  volume = {42},
  number = {2},
  issn = {0014-4819, 1432-1106},
  doi = {10.1007/BF00236911},
  urldate = {2025-02-22},
  abstract = {Human subjects were instructed to point one hand to different visual targets which were randomly sequenced, using a paradigm which allowed two degrees of freedom (shoulder, elbow). The time course of the hand trajectory and the joint angular curves were observed. The latter exhibited patterns which change markedly for different movements, whereas the former preserve similar characteristics (in particular, a single peaked tangential velocity curve). The hypothesis is then formulated that the central command for these movements is formulated in terms of trajectories of the hand in space.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\Admin\Zotero\storage\WKATT2L2\Morasso - 1981 - Spatial control of arm movements.pdf}
}

@article{nathUsingDeepLabCut3D2019a,
  title = {Using {{DeepLabCut}} for {{3D}} Markerless Pose Estimation across Species and Behaviors},
  author = {Nath, Tanmay and Mathis, Alexander and Chen, An Chi and Patel, Amir and Bethge, Matthias and Mathis, Mackenzie Weygandt},
  year = {2019},
  month = jul,
  journal = {Nature Protocols},
  volume = {14},
  number = {7},
  pages = {2152--2176},
  publisher = {Nature Publishing Group},
  issn = {1750-2799},
  doi = {10.1038/s41596-019-0176-0},
  urldate = {2025-10-07},
  abstract = {Noninvasive behavioral tracking of animals during experiments is critical to many scientific pursuits. Extracting the poses of animals without using markers is often essential to measuring behavioral effects in biomechanics, genetics, ethology, and neuroscience. However, extracting detailed poses without markers in dynamically changing backgrounds has been challenging. We recently introduced an open-source toolbox called DeepLabCut that builds on a state-of-the-art human pose-estimation algorithm to allow a user to train a deep neural network with limited training data to precisely track user-defined features that match human labeling accuracy. Here, we provide an updated toolbox, developed as a Python package, that includes new features such as graphical user interfaces (GUIs), performance improvements, and active-learning-based network refinement. We provide a step-by-step procedure for using DeepLabCut that guides the user in creating a tailored, reusable analysis pipeline with a graphical processing unit (GPU) in 1--12 h (depending on frame size). Additionally, we provide Docker environments and Jupyter Notebooks that can be run on cloud resources such as Google Colaboratory.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Behavioural methods,Computational platforms and environments,Learning algorithms,Software},
  file = {C:\Users\Admin\Zotero\storage\XWHPLCTU\Nath et al. - 2019 - Using DeepLabCut for 3D markerless pose estimation across species and behaviors.pdf}
}

@misc{segalinMouseActionRecognition2021,
  title = {The {{Mouse Action Recognition System}} ({{MARS}}): A Software Pipeline for Automated Analysis of Social Behaviors in Mice},
  shorttitle = {The {{Mouse Action Recognition System}} ({{MARS}})},
  author = {Segalin, Cristina and Williams, Jalani and Karigo, Tomomi and Hui, May and Zelikowsky, Moriel and Sun, Jennifer J. and Perona, Pietro and Anderson, David J. and Kennedy, Ann},
  year = {2021},
  month = oct,
  primaryclass = {New Results},
  pages = {2020.07.26.222299},
  publisher = {bioRxiv},
  doi = {10.1101/2020.07.26.222299},
  urldate = {2025-10-02},
  abstract = {The study of naturalistic social behavior requires quantification of animals' interactions. This is generally done through manual annotation---a highly time consuming and tedious process. Recent advances in computer vision enable tracking the pose (posture) of freely-behaving animals. However, automatically and accurately classifying complex social behaviors remains technically challenging. We introduce the Mouse Action Recognition System (MARS), an automated pipeline for pose estimation and behavior quantification in pairs of freely interacting mice. We compare MARS's annotations to human annotations and find that MARS's pose estimation and behavior classification achieve human-level performance. We also release the pose and annotation datasets used to train MARS, to serve as community benchmarks and resources. Finally, we introduce the Behavior Ensemble and Neural Trajectory Observatory (BENTO), a graphical user interface for analysis of multimodal neuroscience datasets. Together, MARS and BENTO provide an end-to-end pipeline for behavior data extraction and analysis, in a package that is user-friendly and easily modifiable.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {C:\Users\Admin\Zotero\storage\XL6DVRGK\Segalin et al. - 2021 - The Mouse Action Recognition System (MARS) a software pipeline for automated analysis of social beh.pdf}
}

@misc{sirmpilatzeNeuroinformaticsunitMovement2024,
  title = {Neuroinformatics-Unit/Movement},
  shorttitle = {Neuroinformatics-Unit/Movement},
  author = {Sirmpilatze, Nikoloz and Huan, Chang and Mi{\~n}ano, Sof{\'i}a and Brandon, D. Peri and Sharma, Dhruv and Porta, Laura and Varela, Iv{\'a}n and Tyson, Adam L.},
  year = {2024},
  doi = {10.5281/zenodo.17160903},
  urldate = {2025-10-08},
  howpublished = {Zenodo},
  file = {C:\Users\Admin\Zotero\storage\WIMRE2P7\17160903.html}
}

@article{sunMultiAgentBehaviorDataset2021b,
  title = {The {{Multi-Agent Behavior Dataset}}: {{Mouse Dyadic Social Interactions}}},
  shorttitle = {The {{Multi-Agent Behavior Dataset}}},
  author = {Sun, Jennifer J. and Karigo, Tomomi and Chakraborty, Dipam and Mohanty, Sharada P. and Wild, Benjamin and Sun, Quan and Chen, Chen and Anderson, David J. and Perona, Pietro and Yue, Yisong and Kennedy, Ann},
  year = {2021},
  month = dec,
  journal = {Advances in neural information processing systems},
  volume = {2021},
  number = {DB1},
  pages = {1--15},
  issn = {1049-5258},
  urldate = {2025-09-27},
  abstract = {Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. To help accelerate behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the performance of automated behavior classification methods in three settings: (1) for training on large behavioral datasets all annotated by a single annotator, (2) for style transfer to learn inter-annotator differences in behavior definitions, and (3) for learning of new behaviors of interest given limited training data. The dataset consists of 6 million frames of unlabeled tracked poses of interacting mice, as well as over 1 million frames with tracked poses and corresponding frame-level behavior annotations. The challenge of our dataset is to be able to classify behaviors accurately using both labeled and unlabeled tracking data, as well as being able to generalize to new settings.},
  pmcid = {PMC11067713},
  pmid = {38706835}
}

@article{tillmannSupplementaryASOiDActivelearning2024,
  title = {Supplementary: {{A-SOiD}}, an Active-Learning Platform for Expert-Guided, Data-Efficient Discovery of Behavior},
  author = {Tillmann, Jens F. and Hsu, Alexander I. and Schwarz, Martin K. and Yttri, Eric A.},
  year = {2024},
  month = apr,
  journal = {Nature Methods},
  volume = {21},
  number = {4},
  pages = {703--711},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-024-02200-1},
  urldate = {2025-06-03},
  langid = {english},
  keywords = {Read},
  file = {C:\Users\Admin\Zotero\storage\8LZRFRQK\Tillmann et al. - 2024 - A-SOiD, an active-learning platform for expert-guided, data-efficient discovery of behavior.pdf}
}

@article{torricelliMotorInvariantsAction2023,
  title = {Motor Invariants in Action Execution and Perception},
  author = {Torricelli, Francesco and Tomassini, Alice and Pezzulo, Giovanni and Pozzo, Thierry and Fadiga, Luciano and D'Ausilio, Alessandro},
  year = {2023},
  month = mar,
  journal = {Physics of Life Reviews},
  volume = {44},
  pages = {13--47},
  issn = {15710645},
  doi = {10.1016/j.plrev.2022.11.003},
  urldate = {2025-02-17},
  abstract = {The nervous system is sensitive to statistical regularities of the external world and forms internal models of these regularities to predict environmental dynamics. Given the inherently social nature of human behavior, being capable of building reliable predictive models of others' actions may be essential for successful interaction. While social prediction might seem to be a daunting task, the study of human motor control has accumulated ample evidence that our movements follow a series of kinematic invariants, which can be used by observers to reduce their uncertainty during social exchanges. Here, we provide an overview of the most salient regularities that shape biological motion, examine the role of these invariants in recognizing others' actions, and speculate that anchoring socially-relevant perceptual decisions to such kinematic invariants provides a key computational advantage for inferring conspecifics' goals and intentions.},
  langid = {english},
  keywords = {Read},
  file = {C:\Users\Admin\Zotero\storage\WBFAZLDK\Torricelli et al. - 2023 - Motor invariants in action execution and perception.pdf}
}

@misc{xieRethinkingSpatiotemporalFeature2018,
  title = {Rethinking {{Spatiotemporal Feature Learning}}: {{Speed-Accuracy Trade-offs}} in {{Video Classification}}},
  shorttitle = {Rethinking {{Spatiotemporal Feature Learning}}},
  author = {Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
  year = {2018},
  month = jul,
  number = {arXiv:1712.04851},
  eprint = {1712.04851},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.04851},
  urldate = {2025-07-07},
  abstract = {Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Read},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\N77RK3AD\\Xie et al. - 2018 - Rethinking Spatiotemporal Feature Learning Speed-Accuracy Trade-offs in Video Classification.pdf;C\:\\Users\\Admin\\Zotero\\storage\\J6VYILGN\\1712.html}
}

@misc{yiASFormerTransformerAction2021,
  title = {{{ASFormer}}: {{Transformer}} for {{Action Segmentation}}},
  shorttitle = {{{ASFormer}}},
  author = {Yi, Fangqiu and Wen, Hongyu and Jiang, Tingting},
  year = {2021},
  month = oct,
  number = {arXiv:2110.08568},
  eprint = {2110.08568},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.08568},
  urldate = {2025-06-24},
  abstract = {Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at {\textbackslash}url\{https://github.com/ChinaYi/ASFormer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Read},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9UMWZBFT\\Yi et al. - 2021 - ASFormer Transformer for Action Segmentation.pdf;C\:\\Users\\Admin\\Zotero\\storage\\HLFJIQWV\\2110.html}
}
