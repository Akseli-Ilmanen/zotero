@misc{3blue1brownAttentionTransformersStepbystep2024,
  title = {Attention in Transformers, Step-by-Step {\textbar} {{Deep Learning Chapter}} 6},
  author = {{3Blue1Brown}},
  year = 2024,
  month = apr,
  urldate = {2025-10-27},
  abstract = {Demystifying attention, the key mechanism inside transformers and LLMs. Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support Special thanks to these supporters: https://www.3blue1brown.com/lessons/a...}
}

@misc{3blue1brownTransformersTechLLMs2024,
  title = {Transformers, the Tech behind {{LLMs}} {\textbar} {{Deep Learning Chapter}} 5},
  author = {{3Blue1Brown}},
  year = 2024,
  month = apr,
  urldate = {2025-10-27},
  abstract = {Breaking down how Large Language Models work, visualizing how data flows through. Instead of sponsored ad reads, these lessons are funded directly by viewers: https://3b1b.co/support}
}

@misc{cambridgeneurotechNeuralProbeData2025,
  type = {Text/Html},
  title = {Neural {{Probe Data}}},
  author = {Cambridge NeuroTech},
  year = 2025,
  month = oct,
  publisher = {Cambridge NeuroTech},
  urldate = {2025-10-27},
  abstract = {Advanced neural interfacing technology (silicon neural probes) for pre-clinical research covering neuroscience, neuroprosthetics and brain-machine interfaces.},
  copyright = {Copyright {\copyright}2025 Cambridge NeuroTech.},
  howpublished = {https://www.cambridgeneurotech.com/neural-probes/neural-probe-data},
  langid = {english},
  file = {C:\Users\aksel\Zotero\storage\K3CDKGR2\neural-probe-data.html}
}

@misc{carreiraQuoVadisAction2018a,
  title = {Quo {{Vadis}}, {{Action Recognition}}? {{A New Model}} and the {{Kinetics Dataset}}},
  shorttitle = {Quo {{Vadis}}, {{Action Recognition}}?},
  author = {Carreira, Joao and Zisserman, Andrew},
  year = 2018,
  month = feb,
  number = {arXiv:1705.07750},
  eprint = {1705.07750},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.07750},
  urldate = {2025-10-31},
  abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9\% on HMDB-51 and 98.0\% on UCF-101.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\aksel\\Zotero\\storage\\MLMGA98X\\Carreira and Zisserman - 2018 - Quo Vadis, Action Recognition A New Model and the Kinetics Dataset.pdf;C\:\\Users\\aksel\\Zotero\\storage\\5L5SKBYY\\1705.html}
}

@article{dingTemporalActionSegmentation2024,
  title = {Temporal {{Action Segmentation}}: {{An Analysis}} of {{Modern Techniques}}},
  shorttitle = {Temporal {{Action Segmentation}}},
  author = {Ding, Guodong and Sener, Fadime and Yao, Angela},
  year = 2024,
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {2},
  pages = {1011--1030},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3327284},
  urldate = {2025-10-30},
  abstract = {Temporal action segmentation (TAS) in videos aims at densely identifying video frames in minutes-long videos with multiple action classes. As a long-range video understanding task, researchers have developed an extended collection of methods and examined their performance using various benchmarks. Despite the rapid growth of TAS techniques in recent years, no systematic survey has been conducted in these sectors. This survey analyzes and summarizes the most significant contributions and trends. In particular, we first examine the task definition, common benchmarks, types of supervision, and prevalent evaluation measures. In addition, we systematically investigate two essential techniques of this topic, i.e., frame representation and temporal modeling, which have been studied extensively in the literature. We then conduct a thorough review of existing TAS works categorized by their levels of supervision and conclude our survey by identifying and emphasizing several research gaps.},
  keywords = {literature survey,Motion segmentation,Semantics,Surveys,Task analysis,Taxonomy,temporal & sequential modeling,Temporal action segmentation,Video on demand,video representation,Videos},
  file = {C\:\\Users\\aksel\\Zotero\\storage\\45PEKYDI\\Ding et al. - 2024 - Temporal Action Segmentation An Analysis of Modern Techniques.pdf;C\:\\Users\\aksel\\Zotero\\storage\\BZ666DHU\\10294187.html}
}

@inproceedings{duFastUnsupervisedAction2022a,
  title = {Fast and {{Unsupervised Action Boundary Detection}} for {{Action Segmentation}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Du, Zexing and Wang, Xue and Zhou, Guoqing and Wang, Qing},
  year = 2022,
  month = jun,
  pages = {3313--3322},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00332},
  urldate = {2025-10-31},
  abstract = {To deal with the great number of untrimmed videos produced every day, we propose an efficient unsupervised action segmentation method by detecting boundaries, named action boundary detection (ABD). In particular, the proposed method has the following advantages: no training stage and low-latency inference. To detect action boundaries, we estimate the similarities across smoothed frames, which inherently have the properties of internal consistency within actions and external discrepancy across actions. Under this circumstance, we successfully transfer the boundary detection task into the change point detection based on the similarity. Then, non-maximum suppression (NMS) is conducted in local windows to select the smallest points as candidate boundaries. In addition, a clustering algorithm is followed to refine the initial proposals. Moreover, we also extend ABD to the online setting, which enables real-time action segmentation in long untrimmed videos. By evaluating on four challenging datasets, our method achieves stateof-the-art performance. Moreover, thanks to the efficiency of ABD, we achieve the best trade-off between the accuracy and the inference time compared with existing unsupervised approaches.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-6946-3},
  langid = {english},
  file = {C:\Users\aksel\Zotero\storage\K4V6ESFC\Du et al. - 2022 - Fast and Unsupervised Action Boundary Detection for Action Segmentation.pdf}
}

@article{friardBORISFreeVersatile2016,
  title = {{{BORIS}}: A Free, Versatile Open-Source Event-Logging Software for Video/Audio Coding and Live Observations},
  shorttitle = {{{BORIS}}},
  author = {Friard, Olivier and Gamba, Marco},
  year = 2016,
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {11},
  pages = {1325--1330},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12584},
  urldate = {2025-10-13},
  abstract = {Quantitative aspects of the study of animal and human behaviour are increasingly relevant to test hypotheses and find empirical support for them. At the same time, photo and video cameras can store a large number of video recordings and are often used to monitor the subjects remotely. Researchers frequently face the need to code considerable quantities of video recordings with relatively flexible software, often constrained by species-specific options or exact settings. BORIS is a free, open-source and multiplatform standalone program that allows a user-specific coding environment to be set for a computer-based review of previously recorded videos or live observations. Being open to user-specific settings, the program allows a project-based ethogram to be defined that can then be shared with collaborators, or can be imported or modified. Projects created in BORIS can include a list of observations, and each observation may include one or two videos (e.g. simultaneous screening of visual stimuli and the subject being tested; recordings from different sides of an aquarium). Once the user has set an ethogram, including state or point events or both, coding can be performed using previously assigned keys on the computer keyboard. BORIS allows definition of an unlimited number of events (states/point events) and subjects. Once the coding process is completed, the program can extract a time-budget or single or grouped observations automatically and present an at-a-glance summary of the main behavioural features. The observation data and time-budget analysis can be exported in many common formats (TSV, CSV, ODF, XLS, SQL and JSON). The observed events can be plotted and exported in various graphic formats (SVG, PNG, JPG, TIFF, EPS and PDF).},
  copyright = {{\copyright} 2016 The Authors. Methods in Ecology and Evolution {\copyright} 2016 British Ecological Society},
  langid = {english},
  keywords = {behaviour coding,behavioural analysis,coding scheme,ethology,observational data,ttime-budget},
  file = {C\:\\Users\\aksel\\Zotero\\storage\\LJ8ZEVGT\\Friard and Gamba - 2016 - BORIS a free, versatile open-source event-logging software for videoaudio coding and live observat.pdf;C\:\\Users\\aksel\\Zotero\\storage\\59UXDLT4\\2041-210X.html}
}

@article{hsuBSOiDOpensourceUnsupervised2021a,
  title = {B-{{SOiD}}, an Open-Source Unsupervised Algorithm for Identification and Fast Prediction of Behaviors},
  author = {Hsu, Alexander I. and Yttri, Eric A.},
  year = 2021,
  month = aug,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5188},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25420-x},
  urldate = {2025-10-30},
  abstract = {Studying naturalistic animal behavior remains a difficult objective. Recent machine learning advances have enabled limb localization; however, extracting behaviors requires ascertaining the spatiotemporal patterns of these positions. To provide a link from poses to actions and their kinematics, we developed B-SOiD -~an open-source, unsupervised algorithm that identifies behavior without user bias. By training a machine classifier on pose pattern statistics clustered using new methods, our approach achieves greatly improved processing speed and the ability to generalize across subjects or labs. Using a frameshift alignment paradigm, B-SOiD overcomes previous temporal resolution barriers. Using only a single, off-the-shelf camera, B-SOiD provides categories of sub-action for trained behaviors and kinematic measures of individual limb trajectories in any animal model. These behavioral and kinematic measures are difficult but critical to obtain, particularly in the study of rodent and other models of pain, OCD, and movement disorders.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Behavioural methods,Diseases of the nervous system,Motor control},
  file = {C:\Users\aksel\Zotero\storage\YFX7N6FP\Hsu and Yttri - 2021 - B-SOiD, an open-source unsupervised algorithm for identification and fast prediction of behaviors.pdf}
}

@misc{iashinVideoFeatures2020a,
  title = {Video {{Features}}},
  author = {Iashin, Vladimir},
  year = 2020,
  urldate = {2025-02-01}
}

@inproceedings{ishikawaAlleviatingOverSegmentationErrors2021,
  title = {Alleviating {{Over-Segmentation Errors}} by {{Detecting Action Boundaries}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Ishikawa, Yuchi and Kasai, Seito and Aoki, Yoshimitsu and Kataoka, Hirokatsu},
  year = 2021,
  pages = {2322--2331},
  urldate = {2025-10-31},
  langid = {english},
  file = {C:\Users\aksel\Zotero\storage\GRTTGGK3\Ishikawa et al. - 2021 - Alleviating Over-Segmentation Errors by Detecting Action Boundaries.pdf}
}

@misc{kayKineticsHumanAction2017,
  title = {The {{Kinetics Human Action Video Dataset}}},
  author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
  year = 2017,
  month = may,
  number = {arXiv:1705.06950},
  eprint = {1705.06950},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.06950},
  urldate = {2025-10-31},
  abstract = {We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\aksel\\Zotero\\storage\\RYRQUELJ\\Kay et al. - 2017 - The Kinetics Human Action Video Dataset.pdf;C\:\\Users\\aksel\\Zotero\\storage\\PN672Z3G\\1705.html}
}

@misc{kozlovaDLC2ActionDeepLearningbased2025,
  title = {{{DLC2Action}}: {{A Deep Learning-based Toolbox}} for {{Automated Behavior Segmentation}}},
  shorttitle = {{{DLC2Action}}},
  author = {Kozlova, Elizaveta and Bonnetto, Andy and Mathis, Alexander},
  year = 2025,
  month = sep,
  primaryclass = {New Results},
  pages = {2025.09.27.678941},
  publisher = {bioRxiv},
  issn = {2692-8205},
  doi = {10.1101/2025.09.27.678941},
  urldate = {2025-10-28},
  abstract = {While expert biologists can annotate complex behaviors from video data, the process remains tedious and time-consuming, creating a bottleneck for efficient behavioral analysis. Here, we present DLC2Action, an open-source Python toolbox that enables automatic behavior annotation from video or estimated 2D/3D pose tracking data. DLC2Action integrates multiple state-of-the-art deep learning architectures, optimized for action segmentation and supports self-supervised learning (SSL) to leverage unlabeled data, boosting performance with limited labeled datasets. Its robust implementation enables efficient hyperparameter optimization, customizable feature extraction, and data handling. We also standardized eight benchmarks and evaluated DLC2Action on five animal behavior datasets, which comprise common behavioral tests in neuroscience, and four human datasets. Overall these datasets span a wide range of contexts from standard laboratory studies to naturalistic cooking. DLC2Action reached strong performance across those benchmarks. To further showcase the tool's versatility, we applied it to Atari gameplay data and found that in certain games the players' eye movements consistently predict their button presses across different subjects. Furthermore, DLC2Action features an intuitive graphical user interface (GUI) for streamlined behavior annotation, active learning, and assessment of model predictions. Diverse pose, video, and annotation formats are supported. Lastly, DLC2Action is modular and thus designed for extensibility, allowing users to integrate new models, dataset features, and methods. The code and benchmarks are available at: https://github.com/amathislab/DLC2action},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {C:\Users\aksel\Zotero\storage\5ZIW25C9\Kozlova et al. - 2025 - DLC2Action A Deep Learning-based Toolbox for Automated Behavior Segmentation.pdf}
}

@article{linCharacterizingStructureMouse2024,
  title = {Characterizing the Structure of Mouse Behavior Using {{Motion Sequencing}}},
  author = {Lin, Sherry and Gillis, Winthrop F. and Weinreb, Caleb and Zeine, Ayman and Jones, Samuel C. and Robinson, Emma M. and Markowitz, Jeffrey and Datta, Sandeep Robert},
  year = 2024,
  month = nov,
  journal = {Nature Protocols},
  volume = {19},
  number = {11},
  pages = {3242--3291},
  publisher = {Nature Publishing Group},
  issn = {1750-2799},
  doi = {10.1038/s41596-024-01015-w},
  urldate = {2025-10-30},
  abstract = {Spontaneous mouse behavior is composed from repeatedly used modules of movement (e.g., rearing, running or grooming) that are flexibly placed into sequences whose content evolves over time. By identifying behavioral modules and the order in which they are expressed, researchers can gain insight into the effect of drugs, genes, context, sensory stimuli and neural activity on natural behavior. Here we present a protocol for performing Motion Sequencing (MoSeq), an ethologically inspired method that uses three-dimensional machine vision and unsupervised machine learning to decompose spontaneous mouse behavior into a series of elemental modules called `syllables'. This protocol is based upon a MoSeq pipeline that includes modules for depth video acquisition, data preprocessing and modeling, as well as a standardized set of visualization tools. Users are provided with instructions and code for building a MoSeq imaging rig and acquiring three-dimensional video of spontaneous mouse behavior for submission to the modeling framework; the outputs of this protocol include syllable labels for each frame of the video data as well as summary plots describing how often each syllable was used and how syllables transitioned from one to the other. In addition, we provide instructions for analyzing and visualizing the outputs of keypoint-MoSeq, a recently developed variant of MoSeq that can identify behavioral motifs from keypoints identified from standard (rather than depth) video. This protocol and the accompanying pipeline significantly lower the bar for users without extensive computational ethology experience to adopt this unsupervised, data-driven approach to characterize mouse behavior.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Behavioural methods,Neuroscience},
  file = {C:\Users\aksel\Zotero\storage\S5MQVT3U\Lin et al. - 2024 - Characterizing the structure of mouse behavior using Motion Sequencing.pdf}
}

@misc{liuDiffusionActionSegmentation2023a,
  title = {Diffusion {{Action Segmentation}}},
  author = {Liu, Daochang and Li, Qiyue and Dinh, AnhDung and Jiang, Tingting and Shah, Mubarak and Xu, Chang},
  year = 2023,
  month = aug,
  number = {arXiv:2303.17959},
  eprint = {2303.17959},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17959},
  urldate = {2025-07-03},
  abstract = {Temporal action segmentation is crucial for understanding long-form videos. Previous works on this task commonly adopt an iterative refinement paradigm by using multi-stage models. We propose a novel framework via denoising diffusion models, which nonetheless shares the same inherent spirit of such iterative refinement. In this framework, action predictions are iteratively generated from random noise with input video features as conditions. To enhance the modeling of three striking characteristics of human actions, including the position prior, the boundary ambiguity, and the relational dependency, we devise a unified masking strategy for the conditioning inputs in our framework. Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposed method achieves superior or comparable results to state-of-the-art methods, showing the effectiveness of a generative approach for action segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,Read},
  file = {C\:\\Users\\aksel\\Zotero\\storage\\5S7K7X79\\Liu et al. - 2023 - Diffusion Action Segmentation.pdf;C\:\\Users\\aksel\\Zotero\\storage\\VQPQVLV9\\2303.html}
}

@article{luxemIdentifyingBehavioralStructure2022,
  title = {Identifying Behavioral Structure from Deep Variational Embeddings of Animal Motion},
  author = {Luxem, Kevin and Mocellin, Petra and Fuhrmann, Falko and K{\"u}rsch, Johannes and Miller, Stephanie R. and Palop, Jorge J. and Remy, Stefan and Bauer, Pavol},
  year = 2022,
  month = nov,
  journal = {Communications Biology},
  volume = {5},
  number = {1},
  pages = {1267},
  publisher = {Nature Publishing Group},
  issn = {2399-3642},
  doi = {10.1038/s42003-022-04080-7},
  urldate = {2025-10-30},
  abstract = {Quantification and detection of the hierarchical organization of behavior is a major challenge in neuroscience. Recent advances in markerless pose estimation enable the visualization of high-dimensional spatiotemporal behavioral dynamics of animal motion. However, robust and reliable technical approaches are needed to uncover underlying structure in these data and to segment behavior into discrete hierarchically organized motifs. Here, we present an unsupervised probabilistic deep learning framework that identifies behavioral structure from deep variational embeddings of animal motion (VAME). By using a mouse model of beta amyloidosis as a use case, we show that VAME not only identifies discrete behavioral motifs, but also captures a hierarchical representation of the motif's usage. The approach allows for the grouping of motifs into communities and the detection of differences in community-specific motif usage of individual mouse cohorts that were undetectable by human visual observation. Thus, we present a robust approach for the segmentation of animal motion that is applicable to a wide range of experimental setups, models and conditions without requiring supervised or a-priori human interference.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Behavioural methods,Computational neuroscience},
  file = {C:\Users\aksel\Zotero\storage\LY8DMF3P\Luxem et al. - 2022 - Identifying behavioral structure from deep variational embeddings of animal motion.pdf}
}

@article{luxemOpensourceToolsBehavioral,
  title = {Open-Source Tools for Behavioral Video Analysis: {{Setup}}, Methods, and Best Practices},
  shorttitle = {Open-Source Tools for Behavioral Video Analysis},
  author = {Luxem, Kevin and Sun, Jennifer J and Bradley, Sean P and Krishnan, Keerthi and Yttri, Eric and Zimmermann, Jan and Pereira, Talmo D and Laubach, Mark},
  journal = {eLife},
  volume = {12},
  pages = {e79305},
  issn = {2050-084X},
  doi = {10.7554/eLife.79305},
  urldate = {2025-03-22},
  abstract = {Recently developed methods for video analysis, especially models for pose estimation and behavior classification, are transforming behavioral quantification to be more precise, scalable, and reproducible in fields such as neuroscience and ethology. These tools overcome long-standing limitations of manual scoring of video frames and traditional `center of mass' tracking algorithms to enable video analysis at scale. The expansion of open-source tools for video acquisition and analysis has led to new experimental approaches to understand behavior. Here, we review currently available open-source tools for video analysis and discuss how to set up these methods for labs new to video recording. We also discuss best practices for developing and using video analysis methods, including community-wide standards and critical needs for the open sharing of datasets and code, more widespread comparisons of video analysis methods, and better documentation for these methods especially for new users. We encourage broader adoption and continued development of these tools, which have tremendous potential for accelerating scientific progress in understanding the brain and behavior.},
  pmcid = {PMC10036114},
  pmid = {36951911},
  keywords = {Maybe Read},
  file = {C:\Users\aksel\Zotero\storage\XNIVU9Q7\Luxem et al. - Open-source tools for behavioral video analysis Setup, methods, and best practices.pdf}
}

@article{mlostEvaluationUnsupervisedLearning2025,
  title = {Evaluation of Unsupervised Learning Algorithms for the Classification of Behavior from Pose Estimation Data},
  author = {Mlost, Jakub and Dawli, Rame and Liu, Xuan and Costa, Ana Rita and Dorocic, Iskra Pollak},
  year = 2025,
  month = may,
  journal = {Patterns},
  volume = {6},
  number = {5},
  publisher = {Elsevier},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2025.101237},
  urldate = {2025-05-15},
  langid = {english},
  keywords = {behavioral classification,Must Read,neuroethology,neuroscience,unsupervised learning},
  file = {C:\Users\aksel\Zotero\storage\7LZEFNWD\Mlost et al. - 2025 - Evaluation of unsupervised learning algorithms for the classification of behavior from pose estimati.pdf}
}

@article{mollLearnedPrecisionTool2025,
  title = {Learned Precision Tool Use in Carrion Crows},
  author = {Moll, Felix W. and W{\"u}rzler, Julius and Nieder, Andreas},
  year = 2025,
  month = oct,
  journal = {Current Biology},
  volume = {35},
  number = {19},
  pages = {4845-4852.e3},
  publisher = {Elsevier},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2025.08.033},
  urldate = {2025-10-13},
  langid = {english},
  pmid = {40934918},
  keywords = {carrion crow,corvid,Corvus corone,pose estimation,skill learning,tool use},
  file = {C:\Users\aksel\Zotero\storage\C6ZHWUI8\Moll et al. - 2025 - Learned precision tool use in carrion crows.pdf}
}

@article{nathUsingDeepLabCut3D2019b,
  title = {Using {{DeepLabCut}} for {{3D}} Markerless Pose Estimation across Species and Behaviors},
  author = {Nath, Tanmay and Mathis, Alexander and Chen, An Chi and Patel, Amir and Bethge, Matthias and Mathis, Mackenzie Weygandt},
  year = 2019,
  month = jul,
  journal = {Nature Protocols},
  volume = {14},
  number = {7},
  pages = {2152--2176},
  publisher = {Nature Publishing Group},
  issn = {1750-2799},
  doi = {10.1038/s41596-019-0176-0},
  urldate = {2025-10-07},
  abstract = {Noninvasive behavioral tracking of animals during experiments is critical to many scientific pursuits. Extracting the poses of animals without using markers is often essential to measuring behavioral effects in biomechanics, genetics, ethology, and neuroscience. However, extracting detailed poses without markers in dynamically changing backgrounds has been challenging. We recently introduced an open-source toolbox called DeepLabCut that builds on a state-of-the-art human pose-estimation algorithm to allow a user to train a deep neural network with limited training data to precisely track user-defined features that match human labeling accuracy. Here, we provide an updated toolbox, developed as a Python package, that includes new features such as graphical user interfaces (GUIs), performance improvements, and active-learning-based network refinement. We provide a step-by-step procedure for using DeepLabCut that guides the user in creating a tailored, reusable analysis pipeline with a graphical processing unit (GPU) in 1--12 h (depending on frame size). Additionally, we provide Docker environments and Jupyter Notebooks that can be run on cloud resources such as Google Colaboratory.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Behavioural methods,Computational platforms and environments,Learning algorithms,Software},
  file = {C:\Users\aksel\Zotero\storage\XWHPLCTU\Nath et al. - 2019 - Using DeepLabCut for 3D markerless pose estimation across species and behaviors.pdf}
}

@article{pachitariuSpikeSortingKilosort42024a,
  title = {Spike Sorting with {{Kilosort4}}},
  author = {Pachitariu, Marius and Sridhar, Shashwat and Pennington, Jacob and Stringer, Carsen},
  year = 2024,
  month = may,
  journal = {Nature Methods},
  volume = {21},
  number = {5},
  pages = {914--921},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02232-7},
  urldate = {2025-10-27},
  abstract = {Spike sorting is the computational process of extracting the firing times of single neurons from recordings of local electrical fields. This is an important but hard problem in neuroscience, made complicated by the nonstationarity of the recordings and the dense overlap in electrical fields between nearby neurons. To address the spike-sorting problem, we have been openly developing the Kilosort framework. Here we describe the various algorithmic steps introduced in different versions of Kilosort. We also report the development of Kilosort4, a version with substantially improved performance due to clustering algorithms inspired by graph-based approaches. To test the performance of Kilosort, we developed a realistic simulation framework that uses densely sampled electrical fields from real experiments to generate nonstationary spike waveforms and realistic noise. We found that nearly all versions of Kilosort outperformed other algorithms on a variety of simulated conditions and that Kilosort4 performed best in all cases, correctly identifying even neurons with low amplitudes and small spatial extents in high drift conditions.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Computational platforms and environments},
  file = {C:\Users\aksel\Zotero\storage\H36Y7FCY\Pachitariu et al. - 2024 - Spike sorting with Kilosort4.pdf}
}

@misc{segalinMouseActionRecognition2021,
  title = {The {{Mouse Action Recognition System}} ({{MARS}}): A Software Pipeline for Automated Analysis of Social Behaviors in Mice},
  shorttitle = {The {{Mouse Action Recognition System}} ({{MARS}})},
  author = {Segalin, Cristina and Williams, Jalani and Karigo, Tomomi and Hui, May and Zelikowsky, Moriel and Sun, Jennifer J. and Perona, Pietro and Anderson, David J. and Kennedy, Ann},
  year = 2021,
  month = oct,
  primaryclass = {New Results},
  pages = {2020.07.26.222299},
  publisher = {bioRxiv},
  doi = {10.1101/2020.07.26.222299},
  urldate = {2025-10-02},
  abstract = {The study of naturalistic social behavior requires quantification of animals' interactions. This is generally done through manual annotation---a highly time consuming and tedious process. Recent advances in computer vision enable tracking the pose (posture) of freely-behaving animals. However, automatically and accurately classifying complex social behaviors remains technically challenging. We introduce the Mouse Action Recognition System (MARS), an automated pipeline for pose estimation and behavior quantification in pairs of freely interacting mice. We compare MARS's annotations to human annotations and find that MARS's pose estimation and behavior classification achieve human-level performance. We also release the pose and annotation datasets used to train MARS, to serve as community benchmarks and resources. Finally, we introduce the Behavior Ensemble and Neural Trajectory Observatory (BENTO), a graphical user interface for analysis of multimodal neuroscience datasets. Together, MARS and BENTO provide an end-to-end pipeline for behavior data extraction and analysis, in a package that is user-friendly and easily modifiable.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {C:\Users\aksel\Zotero\storage\XL6DVRGK\Segalin et al. - 2021 - The Mouse Action Recognition System (MARS) a software pipeline for automated analysis of social beh.pdf}
}

@article{singhaniaC2FTCNFrameworkSemi2023a,
  title = {{{C2F-TCN}}: {{A Framework}} for {{Semi-}} and {{Fully-Supervised Temporal Action Segmentation}}},
  shorttitle = {{{C2F-TCN}}},
  author = {Singhania, Dipika and Rahaman, Rahul and Yao, Angela},
  year = 2023,
  month = oct,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {10},
  pages = {11484--11501},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3284080},
  urldate = {2025-11-01},
  abstract = {Temporal action segmentation tags action labels for every frame in an input untrimmed video containing multiple actions in a sequence. For the task of temporal action segmentation, we propose an encoder-decoder style architecture named C2F-TCN featuring a ``coarse-to-fine'' ensemble of decoder outputs. The C2F-TCN framework is enhanced with a novel model agnostic temporal feature augmentation strategy formed by the computationally inexpensive strategy of the stochastic max-pooling of segments. It produces more accurate and well-calibrated supervised results on three benchmark action segmentation datasets. We show that the architecture is flexible for both supervised and representation learning. In line with this, we present a novel unsupervised way to learn frame-wise representation from C2F-TCN. Our unsupervised learning approach hinges on the clustering capabilities of the input features and the formation of multi-resolution features from the decoder's implicit structure. Further, we provide first semi-supervised temporal action segmentation results by merging representation learning with conventional supervised learning. Our semi-supervised learning scheme, called ``Iterative-Contrastive-Classify (ICC)'', progressively improves in performance with more labeled data. The ICC semi-supervised learning in C2F-TCN, with 40\% labeled videos, performs similar to fully supervised counterparts.},
  keywords = {Computer architecture,Decoding,Image segmentation,Motion segmentation,Representation learning,semi-supervised learning,Task analysis,temporal action segmentation,temporal convolution network,Training,unsupervised representation,Video analysis,video understanding,vision and scene understanding},
  file = {C:\Users\aksel\Zotero\storage\LM427MIV\Singhania et al. - 2023 - C2F-TCN A Framework for Semi- and Fully-Supervised Temporal Action Segmentation.pdf}
}

@article{singhaniaIterativeContrastClassifySemisupervised2022,
  title = {Iterative {{Contrast-Classify}} for {{Semi-supervised Temporal Action Segmentation}}},
  author = {Singhania, Dipika and Rahaman, Rahul and Yao, Angela},
  year = 2022,
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {2},
  pages = {2262--2270},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i2.20124},
  urldate = {2025-10-30},
  abstract = {Temporal action segmentation classifies the action of each frame in (long) video sequences. Due to the high cost of frame-wise labeling, we propose the first semi-supervised method for temporal action segmentation. Our method hinges on unsupervised representation learning, which, for temporal action segmentation, poses unique challenges. Actions in untrimmed videos vary in length and have unknown labels and start/end times. Ordering of actions across videos may also vary. We propose a novel way to learn frame-wise representations from temporal convolutional networks (TCNs) by clustering input features with added time-proximity conditions and multi-resolution similarity. By merging representation learning with conventional supervised learning, we develop an "Iterative Contrast-Classify (ICC)'' semi-supervised learning scheme. With more labelled data, ICC progressively improves in performance; ICC semi-supervised learning, with 40\% labelled videos, performs similarly to fully-supervised counterparts. Our ICC improves MoF by \{+1.8, +5.6, +2.5\}\% on Breakfast, 50Salads, and GTEA respectively for 100\% labelled videos.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Computer Vision (CV),Must Read},
  file = {C:\Users\aksel\Zotero\storage\EGKKJFQP\Singhania et al. - 2022 - Iterative Contrast-Classify for Semi-supervised Temporal Action Segmentation.pdf}
}

@misc{sirmpilatzeNeuroinformaticsunitMovement2024a,
  title = {Neuroinformatics-Unit/Movement},
  shorttitle = {Neuroinformatics-Unit/Movement},
  author = {Sirmpilatze, Nikoloz and Huan, Chang and Mi{\~n}ano, Sof{\'i}a and Brandon, D. Peri and Sharma, Dhruv and Porta, Laura and Varela, Iv{\'a}n and Tyson, Adam L.},
  year = 2024,
  doi = {10.5281/zenodo.17160903},
  urldate = {2025-10-08},
  howpublished = {Zenodo},
  file = {C:\Users\aksel\Zotero\storage\WIMRE2P7\17160903.html}
}

@article{steinmetzNeuropixels20Miniaturized2021a,
  title = {Neuropixels 2.0: {{A}} Miniaturized High-Density Probe for Stable, Long-Term Brain Recordings},
  shorttitle = {Neuropixels 2.0},
  author = {Steinmetz, Nicholas A. and Aydin, Cagatay and Lebedeva, Anna and Okun, Michael and Pachitariu, Marius and Bauza, Marius and Beau, Maxime and Bhagat, Jai and B{\"o}hm, Claudia and Broux, Martijn and Chen, Susu and Colonell, Jennifer and Gardner, Richard J. and Karsh, Bill and Kloosterman, Fabian and Kostadinov, Dimitar and {Mora-Lopez}, Carolina and O'Callaghan, John and Park, Junchol and Putzeys, Jan and Sauerbrei, Britton and {van Daal}, Rik J. J. and Vollan, Abraham Z. and Wang, Shiwei and Welkenhuysen, Marleen and Ye, Zhiwen and Dudman, Joshua T. and Dutta, Barundeb and Hantman, Adam W. and Harris, Kenneth D. and Lee, Albert K. and Moser, Edvard I. and O'Keefe, John and Renart, Alfonso and Svoboda, Karel and H{\"a}usser, Michael and Haesler, Sebastian and Carandini, Matteo and Harris, Timothy D.},
  year = 2021,
  month = apr,
  journal = {Science},
  volume = {372},
  number = {6539},
  pages = {eabf4588},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.abf4588},
  urldate = {2025-10-27},
  abstract = {Measuring the dynamics of neural processing across time scales requires following the spiking of thousands of individual neurons over milliseconds and months. To address this need, we introduce the Neuropixels 2.0 probe together with newly designed analysis algorithms. The probe has more than 5000 sites and is miniaturized to facilitate chronic implants in small mammals and recording during unrestrained behavior. High-quality recordings over long time scales were reliably obtained in mice and rats in six laboratories. Improved site density and arrangement combined with newly created data processing methods enable automatic post hoc correction for brain movements, allowing recording from the same neurons for more than 2 months. These probes and algorithms enable stable recordings from thousands of sites during free behavior, even in small animals such as mice.},
  file = {C:\Users\aksel\Zotero\storage\4NFD7DYF\Steinmetz et al. - 2021 - Neuropixels 2.0 A miniaturized high-density probe for stable, long-term brain recordings.pdf}
}

@article{sunMultiAgentBehaviorDataset2021b,
  title = {The {{Multi-Agent Behavior Dataset}}: {{Mouse Dyadic Social Interactions}}},
  shorttitle = {The {{Multi-Agent Behavior Dataset}}},
  author = {Sun, Jennifer J. and Karigo, Tomomi and Chakraborty, Dipam and Mohanty, Sharada P. and Wild, Benjamin and Sun, Quan and Chen, Chen and Anderson, David J. and Perona, Pietro and Yue, Yisong and Kennedy, Ann},
  year = 2021,
  month = dec,
  journal = {Advances in neural information processing systems},
  volume = {2021},
  number = {DB1},
  pages = {1--15},
  issn = {1049-5258},
  urldate = {2025-09-27},
  abstract = {Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. To help accelerate behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the performance of automated behavior classification methods in three settings: (1) for training on large behavioral datasets all annotated by a single annotator, (2) for style transfer to learn inter-annotator differences in behavior definitions, and (3) for learning of new behaviors of interest given limited training data. The dataset consists of 6 million frames of unlabeled tracked poses of interacting mice, as well as over 1 million frames with tracked poses and corresponding frame-level behavior annotations. The challenge of our dataset is to be able to classify behaviors accurately using both labeled and unlabeled tracking data, as well as being able to generalize to new settings.},
  pmcid = {PMC11067713},
  pmid = {38706835}
}

@article{tillmannASOiDActivelearningPlatform2024a,
  title = {A-{{SOiD}}, an Active-Learning Platform for Expert-Guided, Data-Efficient Discovery of Behavior},
  author = {Tillmann, Jens F. and Hsu, Alexander I. and Schwarz, Martin K. and Yttri, Eric A.},
  year = 2024,
  month = apr,
  journal = {Nature Methods},
  volume = {21},
  number = {4},
  pages = {703--711},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02200-1},
  urldate = {2025-10-30},
  abstract = {To identify and extract naturalistic behavior, two methods have become popular: supervised and unsupervised. Each approach carries its own strengths and weaknesses (for example, user bias, training cost, complexity and action discovery), which the user must consider in their decision. Here, an active-learning platform, A-SOiD, blends these strengths, and in doing so, overcomes several of their inherent drawbacks. A-SOiD iteratively learns user-defined groups with a fraction of the usual training data, while attaining expansive classification through directed unsupervised classification. In socially interacting mice, A-SOiD outperformed standard methods despite requiring 85\% less training data. Additionally, it isolated ethologically distinct mouse interactions via unsupervised classification. We observed similar performance and efficiency using nonhuman primate and human three-dimensional pose data. In both cases, the transparency in A-SOiD's cluster definitions revealed the defining features of the supervised classification through a game-theoretic approach. To facilitate use, A-SOiD comes as an intuitive, open-source interface for efficient segmentation of user-defined behaviors and discovered sub-actions.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Behavioural methods,Software},
  file = {C:\Users\aksel\Zotero\storage\AUDE3PQA\Tillmann et al. - 2024 - A-SOiD, an active-learning platform for expert-guided, data-efficient discovery of behavior.pdf}
}

@article{tillmannSupplementaryASOiDActivelearning2024,
  title = {Supplementary: {{A-SOiD}}, an Active-Learning Platform for Expert-Guided, Data-Efficient Discovery of Behavior},
  author = {Tillmann, Jens F. and Hsu, Alexander I. and Schwarz, Martin K. and Yttri, Eric A.},
  year = 2024,
  month = apr,
  journal = {Nature Methods},
  volume = {21},
  number = {4},
  pages = {703--711},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-024-02200-1},
  urldate = {2025-06-03},
  langid = {english},
  keywords = {Read},
  file = {C:\Users\aksel\Zotero\storage\8LZRFRQK\Tillmann et al. - 2024 - A-SOiD, an active-learning platform for expert-guided, data-efficient discovery of behavior.pdf}
}

@article{torricelliMotorInvariantsAction2023,
  title = {Motor Invariants in Action Execution and Perception},
  author = {Torricelli, Francesco and Tomassini, Alice and Pezzulo, Giovanni and Pozzo, Thierry and Fadiga, Luciano and D'Ausilio, Alessandro},
  year = 2023,
  month = mar,
  journal = {Physics of Life Reviews},
  volume = {44},
  pages = {13--47},
  issn = {15710645},
  doi = {10.1016/j.plrev.2022.11.003},
  urldate = {2025-02-17},
  abstract = {The nervous system is sensitive to statistical regularities of the external world and forms internal models of these regularities to predict environmental dynamics. Given the inherently social nature of human behavior, being capable of building reliable predictive models of others' actions may be essential for successful interaction. While social prediction might seem to be a daunting task, the study of human motor control has accumulated ample evidence that our movements follow a series of kinematic invariants, which can be used by observers to reduce their uncertainty during social exchanges. Here, we provide an overview of the most salient regularities that shape biological motion, examine the role of these invariants in recognizing others' actions, and speculate that anchoring socially-relevant perceptual decisions to such kinematic invariants provides a key computational advantage for inferring conspecifics' goals and intentions.},
  langid = {english},
  keywords = {Read},
  file = {C:\Users\aksel\Zotero\storage\WBFAZLDK\Torricelli et al. - 2023 - Motor invariants in action execution and perception.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and ukasz Kaiser, {\L} and Polosukhin, Illia},
  year = 2017,
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-10-30},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {C:\Users\aksel\Zotero\storage\RAAQJEPU\Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{vonzieglerAnalysisBehavioralFlow2024,
  title = {Analysis of Behavioral Flow Resolves Latent Phenotypes},
  author = {{von Ziegler}, Lukas M. and Roessler, Fabienne K. and Sturman, Oliver and Waag, Rebecca and Privitera, Mattia and Duss, Sian N. and O'Connor, Eoin C. and Bohacek, Johannes},
  year = 2024,
  month = dec,
  journal = {Nature Methods},
  volume = {21},
  number = {12},
  pages = {2376--2387},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02500-6},
  urldate = {2025-10-30},
  abstract = {The accurate detection and quantification of rodent behavior forms a cornerstone of basic biomedical research. Current data-driven approaches, which segment free exploratory behavior into clusters, suffer from low statistical power due to multiple testing, exhibit poor transferability across experiments and fail to exploit the rich behavioral profiles of individual animals. Here we introduce a pipeline to capture each animal's behavioral flow, yielding a single metric based on all observed transitions between clusters. By stabilizing these clusters through machine learning, we ensure data transferability, while dimensionality reduction techniques facilitate detailed analysis of individual animals. We provide a large dataset of 771 behavior recordings of freely moving mice---including stress exposures, pharmacological and brain circuit interventions---to identify hidden treatment effects, reveal subtle variations on the level of individual animals and detect brain processes underlying specific interventions. Our pipeline, compatible with popular clustering methods, substantially enhances statistical power and enables predictions of an animal's future behavior.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Behavioural methods,Diseases of the nervous system,Emotion,Mouse,Software},
  file = {C:\Users\aksel\Zotero\storage\HJIXB2QD\von Ziegler et al. - 2024 - Analysis of behavioral flow resolves latent phenotypes.pdf}
}

@inproceedings{wangBoundaryAwareCascadeNetworks2020,
  title = {Boundary-{{Aware Cascade Networks}} for {{Temporal Action Segmentation}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Wang, Zhenzhi and Gao, Ziteng and Wang, Limin and Li, Zhifeng and Wu, Gangshan},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = 2020,
  pages = {34--51},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58595-2_3},
  abstract = {Identifying human action segments in an untrimmed video is still challenging due to boundary ambiguity and over-segmentation issues. To address these problems, we present a new boundary-aware cascade network by introducing two novel components. First, we devise a new cascading paradigm, called Stage Cascade, to enable our model to have adaptive receptive fields and more confident predictions for ambiguous frames. Second, we design a general and principled smoothing operation, termed as local barrier pooling, to aggregate local predictions by leveraging semantic boundary information. Moreover, these two components can be jointly fine-tuned in an end-to-end manner. We perform experiments on three challenging datasets: 50Salads, GTEA and Breakfast dataset, demonstrating that our framework significantly outperforms the current state-of-the-art methods. The code is available at https://github.com/MCG-NJU/BCN.},
  isbn = {978-3-030-58595-2},
  langid = {english},
  keywords = {Cascade strategy,Must Read,Smoothing operator,Temporal action segmentation,Untrimmed video},
  file = {C:\Users\aksel\Zotero\storage\DUZADVS9\Wang et al. - 2020 - Boundary-Aware Cascade Networks for Temporal Action Segmentation.pdf}
}

@article{wangCrossenhancementTransformerAction2024,
  title = {Cross-Enhancement Transformer for Action Segmentation},
  author = {Wang, Jiahui and Wang, Zhengyou and Zhuang, Shanna and Hao, Yaqian and Wang, Hui},
  year = 2024,
  month = mar,
  journal = {Multimedia Tools and Applications},
  volume = {83},
  number = {9},
  pages = {25643--25656},
  issn = {1573-7721},
  doi = {10.1007/s11042-023-16041-1},
  urldate = {2025-10-14},
  abstract = {Temporal convolutions have been the paradigm of choice in action segmentation, which enhances long-term receptive fields by increasing convolution layers. However, deep convolution layers cause a loss of local information required for frame recognition. To solve the above problem, a new encoder-decoder structure is proposed in this paper, called Cross-Enhancement Transformer. Our approach can be effective learning of temporal structure representation with interactive self-attention mechanism and concatenate each layer convolutional feature maps in encoder with a set of features in decoder produced via self-attention. Therefore, local and global information are excavated in a series of frame actions simultaneously. In addition, a new loss function is proposed to enhance the training process whick can penalize over-segmentation errors. Experiments show that our framework performs state-of-the-art on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities and the Breakfast dataset. Codes are available at https://github.com/Wangjhdeveloper/CETNet.},
  langid = {english},
  keywords = {Action segmentation,Read,Self-attention mechanism,Temporal structure,Transformer},
  file = {C:\Users\aksel\Zotero\storage\38CFDFVQ\Wang et al. - 2024 - Cross-enhancement transformer for action segmentation.pdf}
}

@misc{xieRethinkingSpatiotemporalFeature2018a,
  title = {Rethinking {{Spatiotemporal Feature Learning}}: {{Speed-Accuracy Trade-offs}} in {{Video Classification}}},
  shorttitle = {Rethinking {{Spatiotemporal Feature Learning}}},
  author = {Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
  year = 2018,
  month = jul,
  number = {arXiv:1712.04851},
  eprint = {1712.04851},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.04851},
  urldate = {2025-07-07},
  abstract = {Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Read},
  file = {C\:\\Users\\aksel\\Zotero\\storage\\N77RK3AD\\Xie et al. - 2018 - Rethinking Spatiotemporal Feature Learning Speed-Accuracy Trade-offs in Video Classification.pdf;C\:\\Users\\aksel\\Zotero\\storage\\J6VYILGN\\1712.html}
}

@article{xuChangepointDetectionDeep2025,
  title = {Change-Point Detection with Deep Learning: {{A}} Review},
  shorttitle = {Change-Point Detection with Deep Learning},
  author = {Xu, Ruiyu and Song, Zheren and Wu, Jianguo and Wang, Chao and Zhou, Shiyu},
  year = 2025,
  month = mar,
  journal = {Frontiers of Engineering Management},
  volume = {12},
  number = {1},
  pages = {154--176},
  issn = {2096-0255},
  doi = {10.1007/s42524-025-4109-z},
  urldate = {2025-10-14},
  abstract = {Recent advances in deep learning have led to the creation of various methods for change-point detection (CPD). These methods enhance the ability of CPD techniques to handle complex, high-dimensional data, making them more adaptable and less dependent on strict assumptions about data distributions. CPD methods have also demonstrated high accuracy and have been applied across various fields, including manufacturing, healthcare, activity monitoring, finance, and environmental monitoring. This review provides an overview of how these methods are applied, the data sets they use, and how their performance is evaluated. It also organizes techniques into supervised and unsupervised categories, citing key studies. Finally, we explore ongoing challenges and suggest directions for future research to improve interpretability, generalizability, and real-world implementation.},
  langid = {english},
  keywords = {change-point detection,deep learning,supervised learning,time-series analysis,unsupervised learning},
  file = {C:\Users\aksel\Zotero\storage\X5ZCITZ7\Xu et al. - 2025 - Change-point detection with deep learning A review.pdf}
}

@inproceedings{xuEfficientEffectiveWeaklySupervised2024a,
  title = {Efficient and {{Effective Weakly-Supervised Action Segmentation}} via {{Action-Transition-Aware Boundary Alignment}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Angchi and Zheng, Wei-Shi},
  year = 2024,
  month = jun,
  pages = {18253--18262},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.01728},
  urldate = {2025-10-31},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-5300-6},
  langid = {english},
  file = {C:\Users\aksel\Zotero\storage\A4BLRUII\Xu and Zheng - 2024 - Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary A.pdf}
}

@inproceedings{xuWeaklySupervisedActionSegmentation2018,
  title = {Weakly-{{Supervised Action Segmentation}} with {{Iterative Soft Boundary Assignment}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xu, Chenliang and Ding, Li},
  year = 2018,
  month = jun,
  pages = {6508--6516},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00681},
  urldate = {2025-10-31},
  abstract = {In this work, we address the task of weakly-supervised human action segmentation in long, untrimmed videos. Recent methods have relied on expensive learning models, such as Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM). However, these methods suffer from expensive computational cost, thus are unable to be deployed in large scale. To overcome the limitations, the keys to our design are efficiency and scalability. We propose a novel action modeling framework, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion. The proposed framework is evaluated on two benchmark datasets, Breakfast and Hollywood Extended, with four different evaluation metrics. Extensive experimental results show that our methods achieve competitive or superior performance to state-of-the-art methods.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C:\Users\aksel\Zotero\storage\2T3FJ3VQ\Xu and Ding - 2018 - Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment.pdf}
}

@inproceedings{xuWeaklySupervisedActionSegmentation2018a,
  title = {Weakly-{{Supervised Action Segmentation}} with {{Iterative Soft Boundary Assignment}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xu, Chenliang and Ding, Li},
  year = 2018,
  month = jun,
  pages = {6508--6516},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00681},
  urldate = {2025-10-31},
  abstract = {In this work, we address the task of weakly-supervised human action segmentation in long, untrimmed videos. Recent methods have relied on expensive learning models, such as Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM). However, these methods suffer from expensive computational cost, thus are unable to be deployed in large scale. To overcome the limitations, the keys to our design are efficiency and scalability. We propose a novel action modeling framework, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion. The proposed framework is evaluated on two benchmark datasets, Breakfast and Hollywood Extended, with four different evaluation metrics. Extensive experimental results show that our methods achieve competitive or superior performance to state-of-the-art methods.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C:\Users\aksel\Zotero\storage\9EVWMHKL\Xu and Ding - 2018 - Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment.pdf}
}

@misc{yiASFormerTransformerAction2021a,
  title = {{{ASFormer}}: {{Transformer}} for {{Action Segmentation}}},
  shorttitle = {{{ASFormer}}},
  author = {Yi, Fangqiu and Wen, Hongyu and Jiang, Tingting},
  year = 2021,
  month = oct,
  number = {arXiv:2110.08568},
  eprint = {2110.08568},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.08568},
  urldate = {2025-06-24},
  abstract = {Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at {\textbackslash}url\{https://github.com/ChinaYi/ASFormer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Read},
  file = {C\:\\Users\\aksel\\Zotero\\storage\\9UMWZBFT\\Yi et al. - 2021 - ASFormer Transformer for Action Segmentation.pdf;C\:\\Users\\aksel\\Zotero\\storage\\HLFJIQWV\\2110.html}
}
